# -*- coding: utf-8 -*-

# !pip install tiktoken langchain openai chromadb gspread oauth2client nltk pydantic == 1.10.8

class draft02_protask():
    def __init__(self):
        pass

    def run(self, msg):
        """ДЗ PRO | 2 занятие | Курс по дообучению | Гарант

        Automatically generated by Colaboratory.

        Original file is located at
            https://colab.research.google.com/drive/abcdefgh

        Используя материалы по нейро-контролю качества, определите содержание 2 блоков, в которых алгоритмом оценивается, производилась ли менеджером демонстрация клиенту демо-панели нейронных сетей и панели обучения. Получите отчет по работе менеджера на основании полученных ответов.

        # Запускаем алгоритм
        """

        from langchain.llms import OpenAI
        from langchain.docstore.document import Document
        import requests
        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.vectorstores import Chroma
        from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter, \
            TokenTextSplitter
        from langchain.prompts import PromptTemplate
        import pathlib
        import subprocess
        import tempfile
        import ipywidgets as widgets
        import getpass
        import os
        import gspread
        from oauth2client.service_account import ServiceAccountCredentials
        import re
        # игнорирование предупреждений
        import warnings
        warnings.filterwarnings("ignore")
        import logging
        logging.getLogger("langchain.text_splitter").setLevel(logging.ERROR)
        logging.getLogger("chromadb").setLevel(logging.ERROR)
        from google.colab import output
        import os
        import openai
        import tiktoken
        import re
        import nltk
        nltk.download('punkt')

        class bcolors:
            HEADER = '\033[95m'
            OKBLUE = '\033[94m'
            OKCYAN = '\033[96m'
            OKGREEN = '\033[92m'
            WARNING = '\033[93m'
            FAIL = '\033[91m'
            ENDC = '\033[0m'
            BOLD = '\033[1m'
            UNDERLINE = '\033[4m'

        def set_key():
            password_input = widgets.Password(
                description='Введите пароль:',
                layout=widgets.Layout(width='500px'),
                style={'description_width': 'initial', 'white-space': 'pre-wrap', 'overflow': 'auto'})
            login_button = widgets.Button(description='Авторизация')
            output = widgets.Output()

            def on_button_clicked(_):
                with output:
                    openai.api_key = password_input.value
                    os.environ["OPENAI_API_KEY"] = openai.api_key
                    print(f'{bcolors.OKGREEN}{bcolors.BOLD}Ключ сохранен!{bcolors.ENDC}')
                    password_input.layout.display = 'none'
                    login_button.layout.display = 'none'

            login_button.on_click(on_button_clicked)
            display(widgets.VBox([password_input, login_button, output]))

        def load_document_text(file_path) -> str:
            with open(file_path, 'r') as file:
                text = file.read()
            return text

        def create_search_index(text: str, chunk_size: int, chunk_overlap: int) -> Chroma:
            return create_embedding(text, chunk_size, chunk_overlap)

        def load_prompt(url: str) -> str:
            # Extract the document ID from the URL
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('Invalid Google Docs URL')
            doc_id = match_.group(1)

            # Download the document as plain text
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()
            text = response.text
            return f'{text}'

        def create_embedding(data, chunk_size, chunk_overlap):
            def num_tokens_from_string(string: str, encoding_name: str) -> int:
                """Returns the number of tokens in a text string."""
                encoding = tiktoken.get_encoding(encoding_name)
                num_tokens = len(encoding.encode(string))
                return num_tokens

            source_chunks = []
            splitter = NLTKTextSplitter(chunk_size=chunk_size)
            # splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            # splitter = CharacterTextSplitter(separator='\n\n', chunk_size=chunk_size, chunk_overlap=chunk_overlap)

            for chunk in splitter.split_text(data):
                source_chunks.append(Document(page_content=chunk, metadata={}))

            parts = data.split('\n\n')
            last_speeker = ""
            for part in parts:
                # print('$'+part)
                if " Менеджер: " in part:
                    last_speeker = part[:16]
                elif " Клиент: ":
                    last_speeker = part[:14]
                for i, chunk in enumerate(splitter.split_text(part)):
                    if i == 0:
                        source_chunks.append(Document(page_content=chunk, metadata={}))
                    else:
                        source_chunks.append(Document(page_content=last_speeker + chunk, metadata={}))
            # Создание индексов документа
            search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())

            count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), "cl100k_base")
            # print('\n ===========================================: ')
            # print('Количество токенов в документе :', count_token)
            # print('ЦЕНА запроса:', 0.0004*(count_token/1000), ' $')
            return search_index

        def answer(system, topic, temp=0):
            messages = [
                {"role": "system", "content": system},
                {"role": "user", "content": topic}
            ]

            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo-0301",
                messages=messages,
                temperature=temp
            )

            return completion.choices[0].message.content

        # Функция подсчета токенов для модели эмбеддингов
        def num_tokens_from_string(string: str, encoding_name: str) -> int:
            """Returns the number of tokens in a text string."""
            encoding = tiktoken.get_encoding(encoding_name)
            num_tokens = len(encoding.encode(string))
            return num_tokens

        def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):
            """
            Возвращает количество токенов, используемых списком сообщений.
            """
            try:
                # Пытаемся получить кодировку для выбранной модели
                encoding = tiktoken.encoding_for_model(model)
            except KeyError:
                # Если кодировка для выбранной модели не найдена, используем кодировку "cl100k_base"
                encoding = tiktoken.get_encoding("cl100k_base")
            # Если выбранная модель это "gpt-3.5-turbo-0301"
            if model == "gpt-3.5-turbo-0301":
                # Инициализируем счетчик токенов
                num_tokens = 0
                # Проходимся по каждому сообщению в списке сообщений
                for message in messages:
                    # Каждое сообщение обрамляется токенами <im_start> и <im_end>, а также символами новой строки, всего 4 токена
                    num_tokens += 4
                    # Проходимся по каждому полю в сообщении (ключ и значение)
                    for key, value in message.items():
                        # Считаем количество токенов в значении и добавляем их в счетчик токенов
                        num_tokens += len(encoding.encode(value))
                        # Если ключ это "name", то это означает что роль (role) опущена
                        if key == "name":
                            # Роль всегда требуется и всегда занимает 1 токен, так что вычитаем 1 из счетчика
                            num_tokens += -1
                # Каждый ответ начинается с токена <im_start>assistant, так что добавляем 2 в счетчик
                num_tokens += 2
                # Возвращаем количество токенов
                return num_tokens
            else:
                # Если выбранная модель не поддерживается, генерируем исключение
                raise NotImplementedError(f"""num_tokens_from_messages() is not presently implemented for model {model}.
        See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.""")

        def insert_newlines(text: str, max_len: int = 170) -> str:
            words = text.split()
            lines = []
            current_line = ""
            for word in words:
                if len(current_line + " " + word) > max_len:
                    lines.append(current_line)
                    current_line = ""
                current_line += " " + word
            lines.append(current_line)
            return "\n".join(lines)

        def dialog():
            user = ''
            dialog = ''

            print(f'{bcolors.OKBLUE}{bcolors.BOLD}С чем связан ваш интерес к искусственному интеллекту?{bcolors.ENDC}')

            while user.lower() not in ['stop', 'exit', 'выход']:
                user = input('Клиент: ')
                if user == 'Stop': break

                dialog += '\n\n' + 'Клиент: ' + user
                add_dialog = answer(expert_prompt, user)

                dialog += '\n\n' + 'Менеджер: ' + add_dialog
                print(f'\n{bcolors.OKBLUE}{bcolors.BOLD}Менеджер:{bcolors.ENDC} {insert_newlines(add_dialog)}')
                report = answer(validation_prompt, dialog)
                answer_text = answer(action_prompt, report)

                print(f'\n{bcolors.OKGREEN}{bcolors.BOLD}Отчёт системы:\n {bcolors.ENDC}{report}')
                print(f'\n{bcolors.HEADER}{bcolors.BOLD}Менеджер: {bcolors.ENDC}{insert_newlines(answer_text)}\n\n')

            return dialog

        def get_chatgpt_ansver3(system, topic, search_index, temp=0):

            messages = [
                {"role": "system", "content": system},
                {"role": "user", "content": topic}
            ]

            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo-0301",
                messages=messages,
                temperature=temp
            )
            print('ANSWER : \n', insert_newlines(completion.choices[0].message.content))

        def summarize_questions(dialog):
            # Применяем модель GPT-3 для суммаризации вопросов
            messages = [
                {"role": "system",
                 "content": "Ты - ассистент отдела продаж, основанный на AI. Ты умеешь профессионально суммаризировать присланные тебе диалоги менеджера и клиента. Твоя задача - суммаризировать диалог, который тебе пришел в объем не более 40 слов Если клиент представился, сохрани информацию об имени."},
                {"role": "user",
                 "content": "Суммаризируй следующий диалог менеджера по продажам и клиента: " + " ".join(dialog)}
            ]

            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo-0301",
                messages=messages,
                temperature=0.3,  # Используем более низкую температуру для более определенной суммаризации
                max_tokens=100  # Ограничиваем количество токенов для суммаризации
            )

            return completion.choices[0].message.content

        def answer_user_question(system_doc_text: str, knowledge_base_url: str, topic: str,
                                 instructions: str, temperature: float, verbose: int, k: int, chunk_size: int,
                                 chunk_overlap: int) -> str:

            # system_doc_text = load_document_text(system_doc_url)
            knowledge_base_text = load_document_text(knowledge_base_url)

            # Создаем индексы поиска
            knowledge_base_index = create_search_index(knowledge_base_text, chunk_size, chunk_overlap)
            # Извлечение наиболее похожих отрезков текста из базы знаний и получение ответа модели
            answer_text = answer_index(system_doc_text, topic, instructions, knowledge_base_index, temp=temperature,
                                       verbose=verbose, k=k)

            return answer_text

        def answer_index(system, topic, instructions, search_index, temp=0, verbose=0, k=5):

            # Selecting documents similar to the question
            # docs = search_index.similarity_search(topic, k=k)
            docs = search_index.similarity_search_with_score(topic, k=k)
            if verbose: print('===========================================')
            message_content = re.sub(r'\n{2}', ' ', '\n '.join(
                [f'Отрывок текста №{i + 1}\n=====================\n' + doc[0].page_content + '\n' for i, doc in
                 enumerate(docs)]))
            if verbose: print('message_content:\n ========================================\n',
                              re.sub(r'\n{2}', ' ', '\n '.join([
                                                                   f'\nОтрывок текста №{i + 1} Оценка схожести: {doc[1]}\n=====================\n' +
                                                                   doc[0].page_content + '\n' for i, doc in
                                                                   enumerate(docs)])))

            messages = [
                {"role": "system", "content": system},
                {"role": "user", "content": f"{instructions}\n\nТексты для анализа:\n{message_content}"}
            ]

            if verbose: print('===========================================')
            if verbose: print(
                f"{num_tokens_from_messages(messages, 'gpt-3.5-turbo-0301')} tokens used for the question")

            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo-0301",
                messages=messages,
                temperature=temp
            )

            if verbose: print('\n ===========================================')
            if verbose: print(f'{completion["usage"]["total_tokens"]} total tokens used (question-answer).')
            if verbose: print('===========================================')
            answer = completion.choices[0].message.content
            return answer  # возвращает ответ вместо его вывода

        def answer_user_question_from_answer(system_doc_text: str, instructions: str, answers: str, temperature: float,
                                             verbose: int) -> str:

            # Извлечение наиболее похожих отрезков текста из базы знаний и получение ответа модели
            answer_text = answer_index_from_answer(system_doc_text, instructions, answers, temp=temperature,
                                                   verbose=verbose)

            return answer_text

        def answer_index_from_answer(system, instructions, answers_content, temp=0, verbose=0):

            if verbose: print('===========================================')
            if verbose: print(f'Результаты анализа:\n{answers_content}')
            messages = [
                {"role": "system", "content": system},
                {"role": "user", "content": f"{instructions}\n\nРезультаты анализа:\n{answers_content}"}
            ]

            if verbose: print('===========================================\n')
            if verbose: print(
                f"{num_tokens_from_messages(messages, 'gpt-3.5-turbo-0301')} tokens used for the question")

            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo-0301",
                messages=messages,
                temperature=temp
            )

            if verbose: print('\n ===========================================')
            if verbose: print(f'{completion["usage"]["total_tokens"]} total tokens used (question-answer).')
            if verbose: print('===========================================')
            answer = completion.choices[0].message.content
            return answer  # возвращает ответ вместо его вывода

        from IPython.display import HTML, display

        # Функция переноса теста в output ячейках
        def set_css():
            display(HTML('''
          <style>
            pre {
                white-space: pre-wrap;
            }
          </style>
          '''))

        get_ipython().events.register('pre_run_cell', set_css)

        """# Запускаем GPT"""

        # Получение ключа API от пользователя и установка его как переменной окружения
        openai_key = getpass.getpass("OpenAI API Key:")
        os.environ["OPENAI_API_KEY"] = openai_key
        openai.api_key = openai_key

        """##Запуск диалога"""

        # @title Диалог
        folder_url_yadisk = "https://disk.yandex.ru/d/rqn25YxUwjEymw"  # @param {type:"string"}
        optional_folder_url_yadisk2 = ""  # @param {type:"string"}
        full_url = f'https://getfile.dokpub.com/yandex/get/{folder_url_yadisk}'
        if len(optional_folder_url_yadisk2):
            full_url_opt = f'https://getfile.dokpub.com/yandex/get/{optional_folder_url_yadisk2}'

        temp_dir = 'temp'

        def download_from_url(full_url, temp_dir, opt=False):
            if not os.path.exists(temp_dir):
                os.mkdir(temp_dir)

            with open("temp/tmp.txt", "w") as f:
                f.write(full_url)

            try:
                try:
                    os.remove("temp.zip")
                    if not opt:
                        !rm - R
                        '/content/Audio Record/'
                    !rm - R
                    '/content/temp/Audio Record/'
                except:
                    pass

                !wget - O
                temp.zip - i
                temp / tmp.txt
                if opt:
                    !unzip
                    '/content/temp.zip' - d
                    '/content/temp/'
                else:
                    !unzip
                    '/content/temp.zip' - d
                    '/content/'
                output.clear()
                print('Файлы успешно загружены!')
            except Exception as e:
                print("Ошибка: ", e)

        download_from_url(full_url, temp_dir, opt=False)
        if len(optional_folder_url_yadisk2):
            download_from_url(full_url_opt, temp_dir, opt=True)

        base_text_path = '/content/Audio Record'
        if len(optional_folder_url_yadisk2):
            base_opt_text_path = '/content/temp/Audio Record'
            for first_file in os.listdir(base_text_path):
                file1_path = os.path.join(base_text_path, first_file)
                for second_file in os.listdir(base_opt_text_path):
                    file2_path = os.path.join(base_opt_text_path, second_file)
                    if '_dialog_' in first_file and '_dialog_' in second_file:
                        merge_text_files(file1_path, file2_path)
                    elif '_client_' in first_file and '_client_' in second_file:
                        merge_text_files(file1_path, file2_path)
                    elif '_manager_' in first_file and '_manager_' in second_file:
                        merge_text_files(file1_path, file2_path)
            print("Текстовые файлы двух записей объеденены!")

        text_path = [os.path.join(base_text_path, file) for file in os.listdir(base_text_path) if file.endswith('.txt')]
        base = ''  # Диалог
        base1 = ''  # Client +
        base2 = ''  # Manager +

        if len(text_path) == 3:
            for i_text in text_path:
                if '_dialog_' in i_text:
                    base = i_text
                    print(i_text)
                elif '_client_' in i_text:
                    base1 = i_text
                    print(i_text)
                elif '_manager_' in i_text:
                    base2 = i_text
                    print(i_text)
        else:
            print(
                'Внимание!!! Количество текстовых файлов не соответствуют количеству равному 3 для правильной их обработки.\nПриведите их в правильный вид.')

        # @title Подсчет токенов и их соотношение в диалоге между клиентом и менджером
        num_token_dialog = num_tokens_from_string(load_document_text(base), encoding_name="cl100k_base")
        num_token_client = num_tokens_from_string(load_document_text(base1), encoding_name="cl100k_base")
        num_token_manager = num_tokens_from_string(load_document_text(base2), encoding_name="cl100k_base")
        print('Токенов в Диалог: ', num_token_dialog)
        print('Токенов в Client +: ', num_token_client)
        print('Токенов в Manager +: ', num_token_manager)
        print("Соотношение токенов Клиент/Менеджер %: ", int((num_token_client * 100) / num_token_manager), '/',
              100 - int((num_token_client * 100) / num_token_manager))

        """###Пометки

        **content** - *это выбор из какой базы будет использоваться текст для анализа*

        **temperature** - *это значение чем меньше (ближе, либо равно нулю) чем точнее к контексту, чем ближе к 1 тем больше фантазии*

        **num_fragment** - *это количество отрезков документа которые отбираются по ключевым фразам и передаются в модель для анализа (сейчас 5 так как при нынешней нарезке влезает в модель по количеству токенов в среднем около 3000, но может варьироваться)*

        **system_prompt** - это роль модели и какие то глобальные установки

        **instructions** - *это описание задачи что нужно сделать над отобранным контекстом по ключевым фразам*

        **topicphrase** - *это ключевые слова для отбора отрывков текста в нужном смысле для анализа*
        """

        # @title Демонстация панели нейронных сетей
        content = 'Manager +'  # @param ['Диалог', 'Client +', 'Manager +']
        chunk_size = 200  # @param {type: "slider", min: 200, max: 1024, step:8}
        chunk_overlap = 0  # @param {type: "slider", min: 0, max: 256, step:8}
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 0  # @param {type:"integer"}
        system_prompt = ""  # @param {type:"string"}
        instructions = ""  # @param {type:"string"}
        topicphrase = ''  # @param {type:"string"}

        if content == 'Диалог':
            content_base = base
        elif content == 'Client +':
            content_base = base1
        elif content == 'Manager +':
            content_base = base2

        output1 = answer_user_question(system_prompt, content_base, topicphrase,
                                       instructions, temperature, 1, num_fragment,
                                       chunk_size, chunk_overlap)  # ОБЩИЙ

        print("\nОтвет:\n", output1)

        # @title Демонстрация панели обучения
        content = 'Manager +'  # @param ['Диалог', 'Client +', 'Manager +']
        chunk_size = 968  # @param {type: "slider", min: 200, max: 1024, step:8}
        chunk_overlap = 0  # @param {type: "slider", min: 0, max: 256, step:8}
        temperature = 0.1  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 0  # @param {type:"integer"}
        system_prompt = ""  # @param {type:"string"}
        instructions = ""  # @param {type:"string"}
        topicphrase = ''  # @param {type:"string"}

        if content == 'Диалог':
            content_base = base
        elif content == 'Client +':
            content_base = base1
        elif content == 'Manager +':
            content_base = base2

        output1 = answer_user_question(system_prompt, content_base, topicphrase,
                                       instructions, temperature, 1, num_fragment,
                                       chunk_size, chunk_overlap)  # ОБЩИЙ

        print("\nОтвет:\n", output1)

        # @title Понимание клиента
        temperature = 0.1  # @param {type: "slider", min: 0, max: 1, step:0.1}
        system_prompt = "Ты самый лучший сотрудник отдела контроля качества общения менеджера по продажам и клиента. Менеджер работает в компании, которая продает обучение программированию на python и нейронным сетям."  # @param {type:"string"}
        instructions = "Проанализируй результаты анализа предыдущих выводов по диалогу и дай общий вывод по ним: Проанализируй как качественно менеджер презентовал Демо панель и платформу обучения  Общая оценка качества общения менеджера: 100% -  отличное, 0% - плохое или их нет"  # @param {type:"string"}
        answer_1 = ""  # @param {type:"string"}
        answer_2 = ""  # @param {type:"string"}

        answers = " ".join([f"Анализ №{i + 1}. {q}\n" for i, q in enumerate([answer_1, answer_2]) if len(q)])

        output1 = answer_user_question_from_answer(system_prompt, instructions, answers,
                                                   temperature, 1)

        print("\nОтвет:\n", output1)


if __name__ == '__main__':
    draft02_protask().run("")