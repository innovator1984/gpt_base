# -*- coding: utf-8 -*-

"""
!pip
install
tiktoken
openai
chromadb
gspread
oauth2client
nltk
faiss - cpu
!pip
install - U
deep - translator
"""

class draft05_lesson():
    def __init__(self):
        pass

    def run(self, msg):
        # -*- coding: utf-8 -*-
        """CHAT00_hz00_Вебинар 25 июля. Речевые модели

        Automatically generated by Colaboratory.

        Original file is located at
            https://colab.research.google.com/drive/1aYpX9a_LErgu742DLHcTVs6vc7F4EjAS

        ## ChatGPT like model

        ### Пример как использовать локально встраивание (embeddings) Llama-cpp в LangChain на Saiga 13B
        """

        from langchain.embeddings import LlamaCppEmbeddings
        """
        !wget
        https: // huggingface.co / CRD716 / ggml - vicuna - 1.1 - quantized / resolve / main / ggml - vicuna - 13
        B - 1.1 - q4_0.bin

        !wget
        https: // huggingface.co / IlyaGusev / saiga_7b_lora_llamacpp / resolve / main / ggml - model - q4_1.bin
        """
        # !wget https://huggingface.co/IlyaGusev/saiga_13b_lora_llamacpp/resolve/main/ggml-model-q4_1.bin

        llama_embeddings = LlamaCppEmbeddings(model_path="/content/ggml-vicuna-13B-1.1-q4_0.bin", n_gpu_layers=40,
                                              n_batch=512)

        text = "Это текст тестового документа"

        """Получение вектора одного текста"""

        query_result = llama_embeddings.embed_query(text)

        # Размер вектора
        len(query_result)

        """Получение вектора списка текстов"""

        doc_result = llama_embeddings.embed_documents([text])

        # Размер вектора первого текста
        len(doc_result[0])

        """Разделение текста на отрезки и формирование векторного хранилища"""

        text = """Создание контента и развитие практики


        * В сентябре 2019 года стартовали первые этапы разработки.
        * С сентября 2019 года по май 2020 года проводились индивидуальные занятия.
        * В феврале-апреле 2020 года контент обновлялся при участии двух преподавателей.
        * В апреле 2020 года состоялись первые живые занятия от разработчиков, получившие положительные отзывы от студентов, однако их продолжительность составляла 2,5-3 часа. Вебинары также сталкивались с проблемами, связанными с прерываниями на вопросы студентов и возможной потерей важной информации.
        * В июне 2020 года были приняты постоянные преподаватели.
        * В июне-сентябре 2020 года перешли на использование записанных видеоуроков, что существенно упростило ведение курса. Студенты получили возможность просматривать уроки в ускоренном режиме, экономя свое время, и при этом не теряя качества материала. Однако после записи более 80% контента возникли проблемы с жестким диском компьютера, и все видеоуроки были потеряны, что потребовало их повторной записи.
        * Создание текстовых заданий с подробными комментариями проходило с мая по декабрь 2021 года. В этом процессе участвовала команда из 13 человек, ответственная за разработку текстовых заданий. Видеоуроки переводились в текстовый формат, затем проверялись и предоставлялись фокус-группе для получения обратной связи. После этого тексты передавались копирайтерам для улучшения описания уроков.
        * С мая 2021 года и по настоящее время продолжается работа над созданием дополнительных курсов.


        * В августе 2020 года была создана демо-панелька, включающая в себя 98 примеров нейронных сетей.
        *  С августа 2020 года по апрель 2021 года происходило разработка системы BCI (Brain-Computer Interface).
        * В сентябре 2020 года были впервые привлечены аутсорсные ресурсы для участия в проекте.
        * В рамках проекта были созданы курсы по организации аутсорсинга, которые помогли эффективно управлять работой с внешними специалистами.


        Развитие практики:


        * Наступил момент первых побед на хакатонах, которые начали аккуратно записывать в 2021 году. Заметно, что большинство победителей были гуманитариями.
        * В августе появились стажировки, и Дмитрий считает их самым захватывающим моментом. Это предоставляет возможность пройти практику и получить ценный опыт.
        * В сентябре 2022 года появилась возможность выкупа стоимости обучения, что вызывало некоторую тревогу, но эксперты Гарант были уверены в положительном результате. Группы стали более ограниченными и регулярными.
        * Реальность показывает, как люди с нулевыми знаниями способны создать первый проект всего за 10 дней. Отведенное время для прохождения контента составляет 1,5 месяца, и это впечатляет.


        Интенсивы, Terra и Фреймворки:


         В период с конца 2019 года до начала 2020 года эксперты Гарант регулярно проводили вебинары, посвященные обучению. Позже появились тематические вебинары, которые позволяют углубиться в определенные темы.
         В ноябре 2020 года были запущены новые интенсивные курсы, которые длились три дня, предлагая более интенсивное обучение.
        Декабрь 2020 года отметился созданием Terra AI, и первая версия была запущена в октябре 2021 года. Был разработан специальный курс по Terra, открывающий новые горизонты обучения.
        Непрерывное усовершенствование интенсивных курсов привело к созданию собственных фреймворков, над которыми студенты, успешно завершившие обучение, активно работают в настоящее время.
        Построение системы обучения в Гарант"""

        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain.vectorstores import FAISS

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)
        docs = text_splitter.create_documents([text])

        db = FAISS.from_documents(docs, llama_embeddings)

        query = "Курс по Terra"
        docs = db.similarity_search_with_score(query)

        docs

        text = """Content creation and practice development


        * In September 2019, the first stages of development started.
        * Individual classes were held from September 2019 to May 2020.
        * In February-April 2020, the content was updated with the participation of two teachers.
        * In April 2020, the first live classes from developers took place, which received positive feedback from students, but their duration was 2.5-3 hours. Webinars also faced problems related to interruptions to students' questions and the possible loss of important information.
        * Permanent teachers were accepted in June 2020.
        * In June-September 2020, we switched to the use of recorded video tutorials, which significantly simplified the course management. Students were able to view lessons in accelerated mode, saving their time, and at the same time without losing the quality of the material. However, after recording more than 80% of the content, there were problems with the computer's hard drive, and all the video tutorials were lost, which required them to be re-recorded.
        * The creation of text tasks with detailed comments took place from May to December 2021. This process involved a team of 13 people responsible for the development of text tasks. The video tutorials were translated into text format, then checked and provided to the focus group for feedback. After that, the texts were passed to the copywriters to improve the description of the lessons.
        * From May 2021 to the present, work continues on the creation of additional courses.


        * In August 2020, a demo panel was created, which includes 98 examples of neural networks.
        * From August 2020 to April 2021, the development of the BCI (Brain-Computer Interface) system took place.
        * In September 2020, outsourcing resources were attracted for the first time to participate in the project.
        * Within the framework of the project, outsourcing courses were created, which helped to effectively manage work with external specialists.


        Practice development:


        * The moment of the first hackathon victories has come, which began to be carefully recorded in 2021. It is noticeable that most of the winners were humanitarians.
        * Internships appeared in August, and Dmitry considers them the most exciting moment. This provides an opportunity to practice and gain valuable experience.
        * In September 2022, the possibility of redeeming the cost of training appeared, which caused some alarm, but the UII experts were confident of a positive result. Groups have become more limited and regular.
        * Reality shows how people with zero knowledge are able to create a first project in just 10 days. The allotted time to complete the content is 1.5 months, and this is impressive.


        Intensives, Terra and Frameworks:


         In the period from the end of 2019 to the beginning of 2020, UII experts regularly conducted webinars dedicated to training. Later, thematic webinars appeared that allow you to delve into certain topics.
         In November 2020, new intensive courses were launched that lasted three days, offering more intensive training.
        December 2020 marked the creation of Terra AI, and the first version was launched in October 2021. A special course on Terra has been developed, opening up new horizons of learning.
        Continuous improvement of intensive courses has led to the creation of their own frameworks, on which students who have successfully completed their studies are actively working at the moment.
        Building a learning system in the UII"""

        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain.vectorstores import FAISS

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)
        docs = text_splitter.create_documents([text])

        db = FAISS.from_documents(docs, llama_embeddings)

        query = "When the Terra Ai course was creation?"
        sim_docs = db.similarity_search_with_score(query)

        sim_docs

        docs

        """### Saiga 7B/13B запуск от разработчика

        ##### Установка Saiga из ориг ноута
        """
        """
        !pip
        install
        torch
        sentencepiece \
                accelerate \
                bitsandbytes \
                git + https: // github.com / huggingface / transformers.git @ 15641892985
        b1d77acc74c9065c332cd7c3f7d7f \
                git + https: // github.com / huggingface / peft.git
        """
        from peft import PeftModel, PeftConfig
        from transformers import AutoModelForCausalLM, AutoTokenizer

        # MODEL_NAME = "IlyaGusev/saiga_13b_lora"
        MODEL_NAME = "IlyaGusev/saiga_7b_lora"

        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        config = PeftConfig.from_pretrained(MODEL_NAME)
        model = AutoModelForCausalLM.from_pretrained(
            config.base_model_name_or_path,
            load_in_8bit=True,
            device_map="auto"
        )
        model = PeftModel.from_pretrained(model, MODEL_NAME)
        model.eval()

        DEFAULT_MESSAGE_TEMPLATE = "<s>{role}\n{content}</s>\n"
        DEFAULT_SYSTEM_PROMPT = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься."""
        DEFAULT_START_TOKEN_ID = 1
        DEFAULT_END_TOKEN_ID = 2
        DEFAULT_BOT_TOKEN_ID = 9225

        class Conversation:
            def __init__(
                    self,
                    message_template=DEFAULT_MESSAGE_TEMPLATE,
                    system_prompt=DEFAULT_SYSTEM_PROMPT,
                    start_token_id=DEFAULT_START_TOKEN_ID,
                    end_token_id=DEFAULT_END_TOKEN_ID,
                    bot_token_id=DEFAULT_BOT_TOKEN_ID
            ):
                self.message_template = message_template
                self.start_token_id = start_token_id
                self.end_token_id = end_token_id
                self.bot_token_id = bot_token_id
                self.messages = [{
                    "role": "system",
                    "content": system_prompt
                }]

            def get_end_token_id(self):
                return self.end_token_id

            def get_start_token_id(self):
                return self.start_token_id

            def get_bot_token_id(self):
                return self.bot_token_id

            def add_user_message(self, message):
                self.messages.append({
                    "role": "user",
                    "content": message
                })

            def add_bot_message(self, message):
                self.messages.append({
                    "role": "bot",
                    "content": message
                })

            def get_prompt(self, tokenizer):
                final_text = ""
                for message in self.messages:
                    message_text = self.message_template.format(**message)
                    final_text += message_text
                final_text += tokenizer.decode([self.start_token_id, self.bot_token_id])
                return final_text.strip()

            def expand(self, messages):
                for message in messages:
                    self.messages.append({
                        "role": self.role_mapping.get(message["role"], message["role"]),
                        "content": message["content"]
                    })

        def generate(model, tokenizer, prompt, generation_config):
            data = tokenizer(prompt, return_tensors="pt")
            data = {k: v.to(model.device) for k, v in data.items()}
            output_ids = model.generate(
                **data,
                generation_config=generation_config
            )[0]
            output_ids = output_ids[len(data["input_ids"][0]):]
            output = tokenizer.decode(output_ids, skip_special_tokens=True)
            return output.strip()

        from transformers import GenerationConfig

        generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
        print(generation_config)

        """##### Запуск ответов по базе знаний"""

        # @title Шаг 1. Установка
        """
        !pip
        install
        tiktoken
        langchain
        openai
        chromadb
        gspread
        oauth2client
        nltk
        faiss - cpu
        pydantic == 1.10
        .8
        """
        from langchain.llms import OpenAI
        from langchain.docstore.document import Document
        import requests
        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.vectorstores import Chroma, FAISS
        from langchain.text_splitter import CharacterTextSplitter, NLTKTextSplitter
        from langchain.prompts import PromptTemplate
        import pathlib
        import subprocess
        import tempfile
        import ipywidgets as widgets
        import os
        import gspread
        from oauth2client.service_account import ServiceAccountCredentials
        import re
        # игнорирование предупреждений
        import warnings
        warnings.filterwarnings("ignore")
        import logging
        logging.getLogger("langchain.text_splitter").setLevel(logging.ERROR)
        logging.getLogger("chromadb").setLevel(logging.ERROR)

        import os
        import openai
        import tiktoken
        import re
        import nltk
        nltk.download('punkt')

        class bcolors:
            HEADER = '\033[95m'
            OKBLUE = '\033[94m'
            OKCYAN = '\033[96m'
            OKGREEN = '\033[92m'
            WARNING = '\033[93m'
            FAIL = '\033[91m'
            ENDC = '\033[0m'
            BOLD = '\033[1m'
            UNDERLINE = '\033[4m'

        def set_key():
            password_input = widgets.Password(
                description='Введите пароль:',
                layout=widgets.Layout(width='500px'),
                style={'description_width': 'initial', 'white-space': 'pre-wrap', 'overflow': 'auto'})
            login_button = widgets.Button(description='Авторизация')
            output = widgets.Output()

            def on_button_clicked(_):
                with output:
                    openai.api_key = password_input.value
                    os.environ["OPENAI_API_KEY"] = openai.api_key
                    print(f'{bcolors.OKGREEN}{bcolors.BOLD}Ключ сохранен!{bcolors.ENDC}')
                    password_input.layout.display = 'none'
                    login_button.layout.display = 'none'

            login_button.on_click(on_button_clicked)
            display(widgets.VBox([password_input, login_button, output]))

        def load_document_text(url: str) -> str:
            # Extract the document ID from the URL
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('Invalid Google Docs URL')
            doc_id = match_.group(1)

            # Download the document as plain text
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()
            text = response.text

            return text

        def create_search_index(text: str) -> Chroma:
            return create_embedding(text)

        def create_embedding(data):
            def num_tokens_from_string(string: str, encoding_name: str) -> int:
                """Returns the number of tokens in a text string."""
                encoding = tiktoken.get_encoding(encoding_name)
                num_tokens = len(encoding.encode(string))
                return num_tokens

            source_chunks = []
            # splitter = CharacterTextSplitter(separator="\r\n\r\n\r\n", chunk_size=1024, chunk_overlap=0)
            splitter = NLTKTextSplitter(chunk_size=512)

            for chunk in splitter.split_text(data):
                source_chunks.append(Document(page_content=chunk, metadata={}))

            # Создание индексов документа
            # search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())
            search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings())

            count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), "cl100k_base")
            # print('\n ===========================================: ')
            print('Количество токенов в документе :', count_token)
            print('ЦЕНА запроса:', 0.0001 * (count_token / 1000), ' $')
            return search_index

        def insert_newlines(text: str, max_len: int = 170) -> str:
            words = text.split()
            lines = []
            current_line = ""
            for word in words:
                if len(current_line + " " + word) > max_len:
                    lines.append(current_line)
                    current_line = ""
                current_line += " " + word
            lines.append(current_line)
            return "\n".join(lines)

        def answer_index(system, instruction, topic, search_index, temp=1, verbose=0, k=8):

            # Selecting documents similar to the question
            docs = search_index.similarity_search(topic, k=k)
            if verbose: print('\n ===========================================\n')
            message_content = re.sub(r'\r\n', ' ', '\n '.join(
                [f'\nОтрывок №{i + 1} документа:\n' + doc.page_content + '\n' for i, doc in enumerate(docs)]))
            if verbose: print('message_content :\n ======================================== \n', message_content)

            inp = f"{instruction}.\n\nДокумент с информацией для ответа клиенту:\n{message_content}.\n\nВопрос:\n{topic}\n\nОтвет:"

            conversation = Conversation()
            conversation.add_user_message(inp)
            prompt = conversation.get_prompt(tokenizer)

            completion = generate(model, tokenizer, prompt, generation_config)

            if verbose: print('\n ===========================================: ')
            answer = insert_newlines(completion)
            # print('ANSWER : \n', answer)
            return answer  # возвращает ответ вместо его вывода

        # @title Шаг 1. Запускаем ячейку, и вводим свой ключ авторизации OpenAI для модели эмбеддингов
        set_key()

        # @title Шаг 2. Загружаем базу знаний для ответов
        knowledge_base_url = 'https://docs.google.com/document/d/1sXHb1dYVwyH4M2kFrCk_7TQYZqhDgZXY-xjCIpolA4s/edit'  # @param {type:"string"}
        # Создаем индексы поиска
        knowledge_base_text = load_document_text(knowledge_base_url)
        knowledge_base_index = create_search_index(knowledge_base_text)

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 2  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u041A\u0430\u043A\u0430\u044F \u0432\u0435\u0440\u043E\u044F\u0442\u043D\u043E\u0441\u0442\u044C \u0442\u043E\u0433\u043E, \u0447\u0442\u043E \u043C\u043D\u0435 \u043E\u0434\u043E\u0431\u0440\u044F\u0442 \u0440\u0430\u0441\u0441\u0440\u043E\u0447\u043A\u0443?"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 3  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0442\u0430\u0440\u0438\u0444 \u041F\u0440\u043E\u0434\u0432\u0438\u043D\u0443\u0442\u044B\u0439"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 4  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0441\u0442\u0430\u0436\u0438\u0440\u043E\u0432\u043A\u0438"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        """### Llama.cpp Langchain

        #### Установки Для GPU
        """
        """
        !pip
        install
        langchain == 0.0
        .228
        pydantic == 1.10
        .8

        !CMAKE_ARGS = "-DLLAMA_CUBLAS=on"
        FORCE_CMAKE = 1
        pip
        install
        llama - cpp - python == 0.1
        .69 - -force - reinstall - -upgrade - -no - cache - dir

        # !pip uninstall -y llama-cpp-python
        # !CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.48 --no-cache-dir
        """
        """#### Установки Для CPU"""

        # !pip install llama-cpp-python==0.1.48 langchain pydantic==1.10.8

        """#### Модель Saiga 13B

        ##### Загрузка и настройка модели
        """
        """
        !wget
        https: // huggingface.co / IlyaGusev / saiga_13b_lora_llamacpp / resolve / main / ggml - model - q4_1.bin
        """
        from langchain.llms import LlamaCpp
        from langchain import PromptTemplate, LLMChain
        from langchain.callbacks.manager import CallbackManager
        from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

        template = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься..

        {question}

        Ответ:
        """

        prompt = PromptTemplate(template=template, input_variables=["question"])

        # Callbacks support token-wise streaming
        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        # Verbose is required to pass to the callback manager

        """GPU"""

        n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.
        n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
        n_ctx = 1300
        temperature = 0.0
        max_tokens = 512

        # Make sure the model path is correct for your system!
        llm = LlamaCpp(
            model_path="/content/ggml-model-q4_1.bin",
            n_gpu_layers=n_gpu_layers,
            n_batch=n_batch,
            n_ctx=n_ctx,
            temperature=temperature,
            max_tokens=max_tokens,
            callback_manager=callback_manager,
            verbose=False,
        )

        llm_chain = LLMChain(prompt=prompt, llm=llm)

        """##### Запуск ответов по базе знаний"""

        # @title Шаг 1. Установка библиотек и функций
        """
        !pip
        install
        tiktoken
        openai
        chromadb
        gspread
        oauth2client
        nltk
        faiss - cpu
        !pip
        install - U
        deep - translator
        """
        from langchain.llms import OpenAI
        from langchain.docstore.document import Document
        import requests
        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.vectorstores import Chroma, FAISS
        from langchain.text_splitter import CharacterTextSplitter, NLTKTextSplitter
        from langchain.prompts import PromptTemplate
        import pathlib
        import subprocess
        import tempfile
        import ipywidgets as widgets
        import os
        import gspread
        from oauth2client.service_account import ServiceAccountCredentials
        import re
        # игнорирование предупреждений
        import warnings
        warnings.filterwarnings("ignore")
        import logging
        logging.getLogger("langchain.text_splitter").setLevel(logging.ERROR)
        logging.getLogger("chromadb").setLevel(logging.ERROR)

        import os
        import openai
        import tiktoken
        import re
        import nltk
        nltk.download('punkt')
        from deep_translator import GoogleTranslator

        class bcolors:
            HEADER = '\033[95m'
            OKBLUE = '\033[94m'
            OKCYAN = '\033[96m'
            OKGREEN = '\033[92m'
            WARNING = '\033[93m'
            FAIL = '\033[91m'
            ENDC = '\033[0m'
            BOLD = '\033[1m'
            UNDERLINE = '\033[4m'

        def set_key():
            password_input = widgets.Password(
                description='Введите пароль:',
                layout=widgets.Layout(width='500px'),
                style={'description_width': 'initial', 'white-space': 'pre-wrap', 'overflow': 'auto'})
            login_button = widgets.Button(description='Авторизация')
            output = widgets.Output()

            def on_button_clicked(_):
                with output:
                    openai.api_key = password_input.value
                    os.environ["OPENAI_API_KEY"] = openai.api_key
                    print(f'{bcolors.OKGREEN}{bcolors.BOLD}Ключ сохранен!{bcolors.ENDC}')
                    password_input.layout.display = 'none'
                    login_button.layout.display = 'none'

            login_button.on_click(on_button_clicked)
            display(widgets.VBox([password_input, login_button, output]))

        def load_document_text(url: str) -> str:
            # Extract the document ID from the URL
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('Invalid Google Docs URL')
            doc_id = match_.group(1)

            # Download the document as plain text
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()
            text = response.text

            return text

        def create_search_index(text: str) -> Chroma:
            return create_embedding(text)

        def create_embedding(data):
            def num_tokens_from_string(string: str, encoding_name: str) -> int:
                """Returns the number of tokens in a text string."""
                encoding = tiktoken.get_encoding(encoding_name)
                num_tokens = len(encoding.encode(string))
                return num_tokens

            source_chunks = []
            # splitter = CharacterTextSplitter(separator="\r\n\r\n\r\n", chunk_size=1024, chunk_overlap=0)
            splitter = NLTKTextSplitter(chunk_size=512)

            for chunk in splitter.split_text(data):
                source_chunks.append(Document(page_content=chunk, metadata={}))

            # Создание индексов документа
            # search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())
            search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings())

            count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), "cl100k_base")
            # print('\n ===========================================: ')
            print('Количество токенов в документе :', count_token)
            print('ЦЕНА запроса:', 0.0001 * (count_token / 1000), ' $')
            return search_index

        def insert_newlines(text: str, max_len: int = 170) -> str:
            words = text.split()
            lines = []
            current_line = ""
            for word in words:
                if len(current_line + " " + word) > max_len:
                    lines.append(current_line)
                    current_line = ""
                current_line += " " + word
            lines.append(current_line)
            return "\n".join(lines)

        def answer_index(system, instruction, topic, search_index, temp=1, verbose=0, k=8):
            # Use any translator you like, in this example GoogleTranslator

            # Selecting documents similar to the question
            docs = search_index.similarity_search(topic, k=k)
            if verbose: print('\n ===========================================\n')
            message_content = re.sub(r'\r\n', ' ', '\n '.join(
                [f'\nОтрывок №{i + 1} документа:\n' + doc.page_content + '\n' for i, doc in enumerate(docs)]))

            if verbose: print('instruction:\n ======================================== \n', instruction)
            if verbose: print('topic:\n ======================================== \n', topic)
            if verbose: print('message_content:\n ======================================== \n', message_content)
            inp = f"Инструкция:\n{instruction}.\n\nВопрос:\n{topic}\n\nДокумент с информацией для клиента:\n{message_content}."

            completion = llm_chain.run(inp)

            if verbose: print('\n ===========================================: ')
            answer = insert_newlines(completion)
            # print('ANSWER : \n', answer)
            return answer  # возвращает ответ вместо его вывода

        # @title Шаг 1. Запускаем ячейку, и вводим свой ключ авторизации OpenAI для модели эмбеддингов
        set_key()

        # @title Шаг 2. Загружаем базу знаний для ответов
        knowledge_base_url = 'https://docs.google.com/document/d/1sXHb1dYVwyH4M2kFrCk_7TQYZqhDgZXY-xjCIpolA4s/edit'  # @param {type:"string"}
        # Создаем индексы поиска
        knowledge_base_text = load_document_text(knowledge_base_url)
        knowledge_base_index = create_search_index(knowledge_base_text)

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 2  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u041A\u0430\u043A\u0430\u044F \u0432\u0435\u0440\u043E\u044F\u0442\u043D\u043E\u0441\u0442\u044C \u0442\u043E\u0433\u043E, \u0447\u0442\u043E \u043C\u043D\u0435 \u043E\u0434\u043E\u0431\u0440\u044F\u0442 \u0440\u0430\u0441\u0441\u0440\u043E\u0447\u043A\u0443?"  # @param {type:"string"}

        system = """You are the support manager in the chat of the University of Artificial Intelligence company. The company sells AI courses.
        The company has a large document with all the materials about the company's products. A client asks you a question in a chat, give him an answer
        based on excerpts from this document, try to answer so that the person wants to buy training after the answer. Answer as
        accurately as possible according to the document, do not invent anything from yourself. Never refer to the title of the document or the titles of its excerpts when responding, the client does not have to do anything
        know about the document you are responding to. Always answer in the first person without references to the sources on which you rely.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 2  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0442\u0430\u0440\u0438\u0444 \u041F\u0440\u043E\u0434\u0432\u0438\u043D\u0443\u0442\u044B\u0439"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 2  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0441\u0442\u0430\u0436\u0438\u0440\u043E\u0432\u043A\u0438"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        """#### Модель Llama2 chat 7b

        ##### Загрузка и настройка модели
        """
        """
        !wget
        https: // huggingface.co / TheBloke / Llama - 2 - 7
        B - Chat - GGML / resolve / main / llama - 2 - 7
        b - chat.ggmlv3.q4_1.bin

        !wget
        https: // huggingface.co / TheBloke / Llama - 2 - 13
        B - chat - GGML / resolve / main / llama - 2 - 13
        b - chat.ggmlv3.q4_0.bin
        """
        from langchain.llms import LlamaCpp
        from langchain import PromptTemplate, LLMChain
        from langchain.callbacks.manager import CallbackManager
        from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

        template = '''System: You are the support manager in the chat of the University of Artificial Intelligence company. The company sells AI courses.
        The company has a large document with all the materials about the company's products. A client asks you a question in a chat, give him an answer
        based on excerpts from this document, try to answer so that the person wants to buy training after the answer. Answer as
        accurately as possible according to the document, do not invent anything from yourself. Never refer to the title of the document or the titles of its excerpts when responding, the client does not have to do anything
        know about the document you are responding to. Always answer in the first person without references to the sources on which you rely.\n\n
        User: {question}. Intstruction: {instruction}. Document with information for responding to the client: {content}.\n\n
        Assistant:
        '''

        prompt = PromptTemplate(template=template, input_variables=["instruction", "content", "question"])

        # Callbacks support token-wise streaming
        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        # Verbose is required to pass to the callback manager

        """GPU"""

        n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.
        n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
        n_ctx = 3000
        temperature = 0.0
        max_tokens = 1024

        # Make sure the model path is correct for your system!
        llm = LlamaCpp(
            model_path="/content/llama-2-7b-chat.ggmlv3.q4_1.bin",
            n_gpu_layers=n_gpu_layers,
            n_batch=n_batch,
            n_ctx=n_ctx,
            temperature=temperature,
            max_tokens=max_tokens,
            callback_manager=callback_manager,
            verbose=True,
        )
        llm_chain = LLMChain(prompt=prompt, llm=llm)

        """##### Запуск ответов по базе знаний"""

        # @title Шаг 1. Установка библиотек и функций
        """
        !pip
        install
        tiktoken
        openai
        chromadb
        gspread
        oauth2client
        nltk
        faiss - cpu
        !pip
        install - U
        deep - translator
        """

        from langchain.llms import OpenAI
        from langchain.docstore.document import Document
        import requests
        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.vectorstores import Chroma, FAISS
        from langchain.text_splitter import CharacterTextSplitter, NLTKTextSplitter
        from langchain.prompts import PromptTemplate
        import pathlib
        import subprocess
        import tempfile
        import ipywidgets as widgets
        import os
        import gspread
        from oauth2client.service_account import ServiceAccountCredentials
        import re
        # игнорирование предупреждений
        import warnings
        warnings.filterwarnings("ignore")
        import logging
        logging.getLogger("langchain.text_splitter").setLevel(logging.ERROR)
        logging.getLogger("chromadb").setLevel(logging.ERROR)

        import os
        import openai
        import tiktoken
        import re
        import nltk
        nltk.download('punkt')
        from deep_translator import GoogleTranslator

        class bcolors:
            HEADER = '\033[95m'
            OKBLUE = '\033[94m'
            OKCYAN = '\033[96m'
            OKGREEN = '\033[92m'
            WARNING = '\033[93m'
            FAIL = '\033[91m'
            ENDC = '\033[0m'
            BOLD = '\033[1m'
            UNDERLINE = '\033[4m'

        def set_key():
            password_input = widgets.Password(
                description='Введите пароль:',
                layout=widgets.Layout(width='500px'),
                style={'description_width': 'initial', 'white-space': 'pre-wrap', 'overflow': 'auto'})
            login_button = widgets.Button(description='Авторизация')
            output = widgets.Output()

            def on_button_clicked(_):
                with output:
                    openai.api_key = password_input.value
                    os.environ["OPENAI_API_KEY"] = openai.api_key
                    print(f'{bcolors.OKGREEN}{bcolors.BOLD}Ключ сохранен!{bcolors.ENDC}')
                    password_input.layout.display = 'none'
                    login_button.layout.display = 'none'

            login_button.on_click(on_button_clicked)
            display(widgets.VBox([password_input, login_button, output]))

        def load_document_text(url: str) -> str:
            # Extract the document ID from the URL
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('Invalid Google Docs URL')
            doc_id = match_.group(1)

            # Download the document as plain text
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()
            text = response.text

            return text

        def create_search_index(text: str) -> Chroma:
            return create_embedding(text)

        def create_embedding(data):
            def num_tokens_from_string(string: str, encoding_name: str) -> int:
                """Returns the number of tokens in a text string."""
                encoding = tiktoken.get_encoding(encoding_name)
                num_tokens = len(encoding.encode(string))
                return num_tokens

            source_chunks = []
            # splitter = CharacterTextSplitter(separator="\r\n\r\n\r\n", chunk_size=1024, chunk_overlap=0)
            splitter = NLTKTextSplitter(chunk_size=512)

            for chunk in splitter.split_text(data):
                source_chunks.append(Document(page_content=chunk, metadata={}))

            # Создание индексов документа
            # search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())
            search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings())

            count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), "cl100k_base")
            # print('\n ===========================================: ')
            print('Количество токенов в документе :', count_token)
            print('ЦЕНА запроса:', 0.0001 * (count_token / 1000), ' $')
            return search_index

        def insert_newlines(text: str, max_len: int = 170) -> str:
            words = text.split()
            lines = []
            current_line = ""
            for word in words:
                if len(current_line + " " + word) > max_len:
                    lines.append(current_line)
                    current_line = ""
                current_line += " " + word
            lines.append(current_line)
            return "\n".join(lines)

        def answer_index(system, instruction, topic, search_index, temp=1, verbose=0, k=8):
            # Use any translator you like, in this example GoogleTranslator
            translated_instruction = GoogleTranslator(source='auto', target='en').translate(instruction)
            translated_topic = GoogleTranslator(source='auto', target='en').translate(topic)

            # Selecting documents similar to the question
            docs = search_index.similarity_search(topic, k=k)
            if verbose: print('\n ===========================================\n')
            message_content = re.sub(r'\r\n', ' ', '\n '.join(
                [f'\nОтрывок №{i + 1} документа:\n' + doc.page_content + '\n' for i, doc in enumerate(docs)]))
            translated_message_content = GoogleTranslator(source='auto', target='en').translate(message_content)

            if verbose: print('translated_instruction:\n ======================================== \n',
                              translated_instruction)
            if verbose: print('translated_topic:\n ======================================== \n', translated_topic)
            if verbose: print('translated_message_content:\n ======================================== \n',
                              translated_message_content)

            completion = llm_chain.run({"instruction": translated_instruction, "content": translated_message_content,
                                        "question": translated_topic})
            translated_completion = GoogleTranslator(source='auto', target='ru').translate(completion)

            if verbose: print('\n ===========================================: ')
            answer = insert_newlines(translated_completion)
            # print('ANSWER : \n', answer)
            return answer  # возвращает ответ вместо его вывода

        # @title Шаг 1. Запускаем ячейку, и вводим свой ключ авторизации OpenAI для модели эмбеддингов
        set_key()

        # @title Шаг 2. Загружаем базу знаний для ответов
        knowledge_base_url = 'https://docs.google.com/document/d/1sXHb1dYVwyH4M2kFrCk_7TQYZqhDgZXY-xjCIpolA4s/edit'  # @param {type:"string"}
        # Создаем индексы поиска
        knowledge_base_text = load_document_text(knowledge_base_url)
        knowledge_base_index = create_search_index(knowledge_base_text)

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 4  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430."  # @param {type:"string"}
        user_question = "\u041A\u0430\u043A\u0430\u044F \u0432\u0435\u0440\u043E\u044F\u0442\u043D\u043E\u0441\u0442\u044C \u0442\u043E\u0433\u043E, \u0447\u0442\u043E \u043C\u043D\u0435 \u043E\u0434\u043E\u0431\u0440\u044F\u0442 \u0440\u0430\u0441\u0441\u0440\u043E\u0447\u043A\u0443?"  # @param {type:"string"}

        system = """You are the support manager in the chat of the University of Artificial Intelligence company. The company sells AI courses.
        The company has a large document with all the materials about the company's products. A client asks you a question in a chat, give him an answer
        based on excerpts from this document, try to answer so that the person wants to buy training after the answer. Answer as
        accurately as possible according to the document, do not invent anything from yourself. Never refer to the title of the document or the titles of its excerpts when responding, the client does not have to do anything
        know about the document you are responding to. Always answer in the first person without references to the sources on which you rely.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 4  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0442\u0430\u0440\u0438\u0444 \u041F\u0440\u043E\u0434\u0432\u0438\u043D\u0443\u0442\u044B\u0439"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 4  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0441\u0442\u0430\u0436\u0438\u0440\u043E\u0432\u043A\u0438"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        """#### Vicuna 13B

        ##### Загрузка и настройка модели
        """

        # !wget https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_0.bin
        """
        !wget
        https: // huggingface.co / CRD716 / ggml - vicuna - 1.1 - quantized / resolve / main / ggml - vicuna - 13
        B - 1.1 - q4_0.bin
        """
        # !git lfs install
        # !git clone https://huggingface.co/chharlesonfire/ggml-vicuna-7b-4bit

        from langchain.llms import LlamaCpp
        from langchain import PromptTemplate, LLMChain
        from langchain.callbacks.manager import CallbackManager
        from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

        template = """[Instruction]
        You are the support manager in the chat of the University of Artificial Intelligence company. The company sells AI courses.
        The company has a large document with all the materials about the company's products. A client asks you a question in a chat, give him an answer
        based on excerpts from this document, try to answer so that the person wants to buy training after the answer. Answer as
        accurately as possible according to the document, do not invent anything from yourself. Never refer to the title of the document or the titles of its excerpts when responding, the client does not have to do anything
        know about the document you are responding to. Always answer in the first person without references to the sources on which you rely. {instruction}.

        Document with information for responding to the client:
        {content}

        [Question]
        {question}

        [Answer]

        """

        prompt = PromptTemplate(template=template, input_variables=["instruction", "content", "question"])

        # Callbacks support token-wise streaming
        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        # Verbose is required to pass to the callback manager

        """GPU"""

        n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.
        n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
        n_ctx = 1024
        temperature = 0.0
        max_tokens = 512

        # Make sure the model path is correct for your system!
        llm = LlamaCpp(
            model_path="/content/ggml-vicuna-13B-1.1-q4_0.bin",
            n_gpu_layers=n_gpu_layers,
            n_batch=n_batch,
            n_ctx=n_ctx,
            temperature=temperature,
            max_tokens=max_tokens,
            callback_manager=callback_manager,
            verbose=False,
        )

        llm_chain = LLMChain(prompt=prompt, llm=llm)

        """##### Запуск ответов по базе знаний"""

        # @title Шаг 1. Установка библиотек и функций
        """
        !pip
        install
        tiktoken
        openai
        chromadb
        gspread
        oauth2client
        nltk
        faiss - cpu
        !pip
        install - U
        deep - translator
        """
        from langchain.llms import OpenAI
        from langchain.docstore.document import Document
        import requests
        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.vectorstores import Chroma, FAISS
        from langchain.text_splitter import CharacterTextSplitter, NLTKTextSplitter
        from langchain.prompts import PromptTemplate
        import pathlib
        import subprocess
        import tempfile
        import ipywidgets as widgets
        import os
        import gspread
        from oauth2client.service_account import ServiceAccountCredentials
        import re
        # игнорирование предупреждений
        import warnings
        warnings.filterwarnings("ignore")
        import logging
        logging.getLogger("langchain.text_splitter").setLevel(logging.ERROR)
        logging.getLogger("chromadb").setLevel(logging.ERROR)

        import os
        import openai
        import tiktoken
        import re
        import nltk
        nltk.download('punkt')
        from deep_translator import GoogleTranslator

        class bcolors:
            HEADER = '\033[95m'
            OKBLUE = '\033[94m'
            OKCYAN = '\033[96m'
            OKGREEN = '\033[92m'
            WARNING = '\033[93m'
            FAIL = '\033[91m'
            ENDC = '\033[0m'
            BOLD = '\033[1m'
            UNDERLINE = '\033[4m'

        def set_key():
            password_input = widgets.Password(
                description='Введите пароль:',
                layout=widgets.Layout(width='500px'),
                style={'description_width': 'initial', 'white-space': 'pre-wrap', 'overflow': 'auto'})
            login_button = widgets.Button(description='Авторизация')
            output = widgets.Output()

            def on_button_clicked(_):
                with output:
                    openai.api_key = password_input.value
                    os.environ["OPENAI_API_KEY"] = openai.api_key
                    print(f'{bcolors.OKGREEN}{bcolors.BOLD}Ключ сохранен!{bcolors.ENDC}')
                    password_input.layout.display = 'none'
                    login_button.layout.display = 'none'

            login_button.on_click(on_button_clicked)
            display(widgets.VBox([password_input, login_button, output]))

        def load_document_text(url: str) -> str:
            # Extract the document ID from the URL
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('Invalid Google Docs URL')
            doc_id = match_.group(1)

            # Download the document as plain text
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()
            text = response.text

            return text

        def create_search_index(text: str) -> Chroma:
            return create_embedding(text)

        def create_embedding(data):
            def num_tokens_from_string(string: str, encoding_name: str) -> int:
                """Returns the number of tokens in a text string."""
                encoding = tiktoken.get_encoding(encoding_name)
                num_tokens = len(encoding.encode(string))
                return num_tokens

            source_chunks = []
            # splitter = CharacterTextSplitter(separator="\r\n\r\n\r\n", chunk_size=1024, chunk_overlap=0)
            splitter = NLTKTextSplitter(chunk_size=512)

            for chunk in splitter.split_text(data):
                source_chunks.append(Document(page_content=chunk, metadata={}))

            # Создание индексов документа
            # search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())
            search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings())

            count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), "cl100k_base")
            # print('\n ===========================================: ')
            print('Количество токенов в документе :', count_token)
            print('ЦЕНА запроса:', 0.0001 * (count_token / 1000), ' $')
            return search_index

        def insert_newlines(text: str, max_len: int = 170) -> str:
            words = text.split()
            lines = []
            current_line = ""
            for word in words:
                if len(current_line + " " + word) > max_len:
                    lines.append(current_line)
                    current_line = ""
                current_line += " " + word
            lines.append(current_line)
            return "\n".join(lines)

        def answer_index(system, instruction, topic, search_index, temp=1, verbose=0, k=8):
            # Use any translator you like, in this example GoogleTranslator
            translated_instruction = GoogleTranslator(source='auto', target='en').translate(instruction)
            translated_topic = GoogleTranslator(source='auto', target='en').translate(topic)

            # Selecting documents similar to the question
            docs = search_index.similarity_search(topic, k=k)
            if verbose: print('\n ===========================================\n')
            message_content = re.sub(r'\r\n', ' ', '\n '.join(
                [f'\nОтрывок №{i + 1} документа:\n' + doc.page_content + '\n' for i, doc in enumerate(docs)]))
            translated_message_content = GoogleTranslator(source='auto', target='en').translate(message_content)

            if verbose: print('translated_instruction:\n ======================================== \n',
                              translated_instruction)
            if verbose: print('translated_topic:\n ======================================== \n', translated_topic)
            if verbose: print('translated_message_content:\n ======================================== \n',
                              translated_message_content)

            completion = llm_chain.run({"instruction": translated_instruction, "content": translated_message_content,
                                        "question": translated_topic})
            translated_completion = GoogleTranslator(source='auto', target='ru').translate(completion)

            if verbose: print('\n ===========================================: ')
            answer = insert_newlines(translated_completion)
            # print('ANSWER : \n', answer)
            return answer  # возвращает ответ вместо его вывода

        # @title Шаг 1. Запускаем ячейку, и вводим свой ключ авторизации OpenAI для модели эмбеддингов
        set_key()

        # @title Шаг 2. Загружаем базу знаний для ответов
        knowledge_base_url = 'https://docs.google.com/document/d/1sXHb1dYVwyH4M2kFrCk_7TQYZqhDgZXY-xjCIpolA4s/edit'  # @param {type:"string"}
        # Создаем индексы поиска
        knowledge_base_text = load_document_text(knowledge_base_url)
        knowledge_base_index = create_search_index(knowledge_base_text)

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 2  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u041A\u0430\u043A\u0430\u044F \u0432\u0435\u0440\u043E\u044F\u0442\u043D\u043E\u0441\u0442\u044C \u0442\u043E\u0433\u043E, \u0447\u0442\u043E \u043C\u043D\u0435 \u043E\u0434\u043E\u0431\u0440\u044F\u0442 \u0440\u0430\u0441\u0441\u0440\u043E\u0447\u043A\u0443?"  # @param {type:"string"}

        system = """You are the support manager in the chat of the University of Artificial Intelligence company. The company sells AI courses.
        The company has a large document with all the materials about the company's products. A client asks you a question in a chat, give him an answer
        based on excerpts from this document, try to answer so that the person wants to buy training after the answer. Answer as
        accurately as possible according to the document, do not invent anything from yourself. Never refer to the title of the document or the titles of its excerpts when responding, the client does not have to do anything
        know about the document you are responding to. Always answer in the first person without references to the sources on which you rely.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 3  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0442\u0430\u0440\u0438\u0444 \u041F\u0440\u043E\u0434\u0432\u0438\u043D\u0443\u0442\u044B\u0439"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 2  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0441\u0442\u0430\u0436\u0438\u0440\u043E\u0432\u043A\u0438"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        """#### Модель Alpaca 13B

        ##### Загрузка и настройка модели
        """
        """
        !wget
        https: // huggingface.co / TheBloke / gpt4 - x - alpaca - 13
        B - GGML / resolve / main / gpt4 - x - alpaca - 13
        b.ggmlv3.q4_1.bin
        """
        from langchain.llms import LlamaCpp
        from langchain import PromptTemplate, LLMChain
        from langchain.callbacks.manager import CallbackManager
        from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

        template = """### Instruction: {question}\n### Response:
        """

        prompt = PromptTemplate(template=template, input_variables=["question"])

        # Callbacks support token-wise streaming
        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        # Verbose is required to pass to the callback manager

        """GPU"""

        n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.
        n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
        n_ctx = 1356
        temperature = 0.0
        max_tokens = 512

        # Make sure the model path is correct for your system!
        llm = LlamaCpp(
            model_path="/content/gpt4-x-alpaca-13b.ggmlv3.q4_1.bin",
            n_gpu_layers=n_gpu_layers,
            n_batch=n_batch,
            n_ctx=n_ctx,
            temperature=temperature,
            max_tokens=max_tokens,
            callback_manager=callback_manager,
            verbose=False,
        )

        llm_chain = LLMChain(prompt=prompt, llm=llm)

        """##### Запуск ответов по базе знаний"""

        # @title Шаг 1. Установка библиотек и функций


        from langchain.llms import OpenAI
        from langchain.docstore.document import Document
        import requests
        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.vectorstores import Chroma, FAISS
        from langchain.text_splitter import CharacterTextSplitter, NLTKTextSplitter
        from langchain.prompts import PromptTemplate
        import pathlib
        import subprocess
        import tempfile
        import ipywidgets as widgets
        import os
        import gspread
        from oauth2client.service_account import ServiceAccountCredentials
        import re
        # игнорирование предупреждений
        import warnings
        warnings.filterwarnings("ignore")
        import logging
        logging.getLogger("langchain.text_splitter").setLevel(logging.ERROR)
        logging.getLogger("chromadb").setLevel(logging.ERROR)

        import os
        import openai
        import tiktoken
        import re
        import nltk
        nltk.download('punkt')
        from deep_translator import GoogleTranslator

        class bcolors:
            HEADER = '\033[95m'
            OKBLUE = '\033[94m'
            OKCYAN = '\033[96m'
            OKGREEN = '\033[92m'
            WARNING = '\033[93m'
            FAIL = '\033[91m'
            ENDC = '\033[0m'
            BOLD = '\033[1m'
            UNDERLINE = '\033[4m'

        def set_key():
            password_input = widgets.Password(
                description='Введите пароль:',
                layout=widgets.Layout(width='500px'),
                style={'description_width': 'initial', 'white-space': 'pre-wrap', 'overflow': 'auto'})
            login_button = widgets.Button(description='Авторизация')
            output = widgets.Output()

            def on_button_clicked(_):
                with output:
                    openai.api_key = password_input.value
                    os.environ["OPENAI_API_KEY"] = openai.api_key
                    print(f'{bcolors.OKGREEN}{bcolors.BOLD}Ключ сохранен!{bcolors.ENDC}')
                    password_input.layout.display = 'none'
                    login_button.layout.display = 'none'

            login_button.on_click(on_button_clicked)
            display(widgets.VBox([password_input, login_button, output]))

        def load_document_text(url: str) -> str:
            # Extract the document ID from the URL
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('Invalid Google Docs URL')
            doc_id = match_.group(1)

            # Download the document as plain text
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()
            text = response.text

            return text

        def create_search_index(text: str) -> Chroma:
            return create_embedding(text)

        def create_embedding(data):
            def num_tokens_from_string(string: str, encoding_name: str) -> int:
                """Returns the number of tokens in a text string."""
                encoding = tiktoken.get_encoding(encoding_name)
                num_tokens = len(encoding.encode(string))
                return num_tokens

            source_chunks = []
            # splitter = CharacterTextSplitter(separator="\r\n\r\n\r\n", chunk_size=1024, chunk_overlap=0)
            splitter = NLTKTextSplitter(chunk_size=512)

            for chunk in splitter.split_text(data):
                source_chunks.append(Document(page_content=chunk, metadata={}))

            # Создание индексов документа
            # search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings())
            search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings())

            count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), "cl100k_base")
            # print('\n ===========================================: ')
            print('Количество токенов в документе :', count_token)
            print('ЦЕНА запроса:', 0.0001 * (count_token / 1000), ' $')
            return search_index

        def insert_newlines(text: str, max_len: int = 170) -> str:
            words = text.split()
            lines = []
            current_line = ""
            for word in words:
                if len(current_line + " " + word) > max_len:
                    lines.append(current_line)
                    current_line = ""
                current_line += " " + word
            lines.append(current_line)
            return "\n".join(lines)

        def answer_index(system, instruction, topic, search_index, temp=1, verbose=0, k=8):
            # Use any translator you like, in this example GoogleTranslator
            translated_instruction = GoogleTranslator(source='auto', target='en').translate(instruction)
            translated_topic = GoogleTranslator(source='auto', target='en').translate(topic)

            # Selecting documents similar to the question
            docs = search_index.similarity_search(topic, k=k)
            if verbose: print('\n ===========================================\n')
            message_content = re.sub(r'\r\n', ' ', '\n '.join(
                [f'\nОтрывок №{i + 1} документа:\n' + doc.page_content + '\n' for i, doc in enumerate(docs)]))
            translated_message_content = GoogleTranslator(source='auto', target='en').translate(message_content)

            if verbose: print('translated_instruction:\n ======================================== \n',
                              translated_instruction)
            if verbose: print('translated_topic:\n ======================================== \n', translated_topic)
            if verbose: print('translated_message_content:\n ======================================== \n',
                              translated_message_content)
            inp = f"{system}\n{translated_instruction}.\n\nDocument with information for responding to the client:\n{translated_message_content}.\n\nQuestion:\n{translated_topic}"

            completion = llm_chain.run(inp)
            translated_completion = GoogleTranslator(source='auto', target='ru').translate(completion)

            if verbose: print('\n ===========================================: ')
            answer = insert_newlines(translated_completion)
            # print('ANSWER : \n', answer)
            return answer  # возвращает ответ вместо его вывода

        # @title Шаг 1. Запускаем ячейку, и вводим свой ключ авторизации OpenAI для модели эмбеддингов
        set_key()

        # @title Шаг 2. Загружаем базу знаний для ответов
        knowledge_base_url = 'https://docs.google.com/document/d/1sXHb1dYVwyH4M2kFrCk_7TQYZqhDgZXY-xjCIpolA4s/edit'  # @param {type:"string"}
        # Создаем индексы поиска
        knowledge_base_text = load_document_text(knowledge_base_url)
        knowledge_base_index = create_search_index(knowledge_base_text)

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 2  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u041A\u0430\u043A\u0430\u044F \u0432\u0435\u0440\u043E\u044F\u0442\u043D\u043E\u0441\u0442\u044C \u0442\u043E\u0433\u043E, \u0447\u0442\u043E \u043C\u043D\u0435 \u043E\u0434\u043E\u0431\u0440\u044F\u0442 \u0440\u0430\u0441\u0441\u0440\u043E\u0447\u043A\u0443?"  # @param {type:"string"}

        system = """You are the support manager in the chat of the University of Artificial Intelligence company. The company sells AI courses.
        The company has a large document with all the materials about the company's products. A client asks you a question in a chat, give him an answer
        based on excerpts from this document, try to answer so that the person wants to buy training after the answer. Answer as
        accurately as possible according to the document, do not invent anything from yourself. Never refer to the title of the document or the titles of its excerpts when responding, the client does not have to do anything
        know about the document you are responding to. Always answer in the first person without references to the sources on which you rely.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 2  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0442\u0430\u0440\u0438\u0444 \u041F\u0440\u043E\u0434\u0432\u0438\u043D\u0443\u0442\u044B\u0439"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))

        # @title Шаг 3. Генерация ответа на вопрос с поиском контекста из Базы знаний Гарант (для повторного запуска ячейки, выполнение предыдущих шагов не нужно)
        verbose = "1"  # @param [0, 1]
        temperature = 0  # @param {type: "slider", min: 0, max: 1, step:0.1}
        num_fragment = 2  # @param {type: "slider", min: 2, max: 4, step:1}
        instruction = "\u041F\u0440\u043E\u0430\u043D\u0430\u043B\u0438\u0437\u0438\u0440\u0443\u0439 \u0434\u043E\u043A\u0443\u043C\u0435\u043D\u0442 \u0441 \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0435\u0439, \u0438 \u0434\u0430\u0439 \u0434\u0435\u0442\u0430\u043B\u044C\u043D\u044B\u0439 \u0438 \u043A\u043E\u0440\u0440\u0435\u043A\u0442\u043D\u044B\u0439 \u043E\u0442\u0432\u0435\u0442 \u043D\u0430 \u0432\u043E\u043F\u0440\u043E\u0441 \u043A\u043B\u0438\u0435\u043D\u0442\u0430"  # @param {type:"string"}
        user_question = "\u0420\u0430\u0441\u0441\u043A\u0430\u0436\u0438 \u043F\u0440\u043E \u0441\u0442\u0430\u0436\u0438\u0440\u043E\u0432\u043A\u0438"  # @param {type:"string"}

        system = """Ты менеджер поддержки в чате компании Гуманизация Исскуственного интеллекта. Компания продает курсы по AI.
        У компании есть большой документ со всеми материалами о продуктах компании. Тебе задает вопрос клиент в чате, дай ему ответ,
        опираясь на отрывки из этого документа, постарайся ответить так, чтобы человек захотел после ответа купить обучение. Отвечай максимально
        точно по документу, не придумывай ничего от себя. Никогда не ссылайся на название документа или названия его отрывков при ответе, клиент ничего не должен
        знать о документе, по которому ты отвечаешь. Всегда отвечай от первого лица без ссылок на источники на которые ты опираешься.
        """
        print(answer_index(system, instruction, user_question, knowledge_base_index, temperature, int(verbose),
                           num_fragment))


if __name__ == '__main__':
    draft05_lesson().run("")