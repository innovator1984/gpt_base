# -*- coding: utf-8 -*-

# !pip install tiktoken == 0.4.0 langchain == 0.0.231 openai == 0.27.8 faiss - cpu == 1.7.4 gspread oauth2client nltk pydantic == 1.10.8

class draft03_litetask():
    def __init__(self):
        pass

    def run(self, msg):
        """ДЗ Light | 3 занятие | Курс по дообучению | Гарант.ipynb

        Automatically generated by Colaboratory.

        Original file is located at
            https://colab.research.google.com/drive/abcdefgh

        Возьмите инструкцию по эксплуатации по ссылке https://drive.google.com/file/d/abcdefgh/view?usp=sharing и на ее основе сделайте базу знаний для ответа на вопросы по этой инструкции. Для этого разбейте взятую инструкцию на логические блоки (как в занятии), и уже эти логические блоки разбейте сплиттерами на чанки. При использовании созданной базы знаний не забудьте открыть доступ к документу. Это-задача-минимум. Задача-максимум - создать нейро-консультанта по данной базе знаний - с поддержанием диалога или без такового - на ваше усмотрение. Проверьте работу созданного нейро-консультанта.
        """

        # @title Установка библиотек


        # @title Импорт библиотек
        import gdown
        from langchain.llms import OpenAI
        from langchain.docstore.document import Document
        import requests
        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.vectorstores import FAISS
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain.prompts import PromptTemplate
        import pathlib
        import subprocess
        import tempfile
        import ipywidgets as widgets
        import os
        import gspread
        from oauth2client.service_account import ServiceAccountCredentials
        import re
        import getpass
        import os
        import openai
        import tiktoken

        # Получение ключа API от пользователя и установка его как переменной окружения
        openai_key = getpass.getpass("OpenAI API Key:")
        os.environ["OPENAI_API_KEY"] = openai_key
        openai.api_key = openai_key

        def split_text(text, max_count, count_type, verbose=0):
            # Функция для подсчета количества слов в фрагменте
            def num_words(fragment):
                return len(fragment.split())

            # Функция для подсчета количества токенов в фрагменте
            def num_tokens(fragment):
                return num_tokens_from_string(fragment, "cl100k_base")

            # Разделение текста на фрагменты, исключая теги HTML
            fragments = [fragment.strip() for fragment in re.split(r"<[^>]+>|[\ufeff]", text) if fragment.strip()]

            # Выбор функции подсчета длины в зависимости от типа подсчета
            length_function = num_words if count_type == "words" else num_tokens

            # Создание объекта разделителя текста
            splitter = RecursiveCharacterTextSplitter(chunk_size=max_count, chunk_overlap=0,
                                                      length_function=length_function)

            # Список для хранения фрагментов текста
            source_chunks = []

            # Обработка каждого фрагмента текста
            for fragment in fragments:
                if verbose:
                    # Вывод количества слов/токенов в фрагменте, если включен режим verbose
                    count = length_function(fragment)
                    print(f"{count_type} in text fragment = {count}\n{'-' * 5}\n{fragment}\n{'=' * 20}")

                # Разбиение фрагмента текста на части заданной длины с помощью разделителя
                # и добавление каждой части в список source_chunks
                # source_chunks.append(Document(page_content=fragment, metadata={}) for chunk in splitter.split_text(fragment))
                source_chunks.extend(
                    Document(page_content=chunk, metadata={}) for chunk in splitter.split_text(fragment))

            # Возвращение списка фрагментов текста
            return source_chunks

        def create_embedding(data, max_count, count_type):
            def num_tokens_from_string(string: str, encoding_name: str) -> int:
                """Возвращает количество токенов в строке"""
                encoding = tiktoken.get_encoding(encoding_name)
                num_tokens = len(encoding.encode(string))
                return num_tokens

            source_chunks = []

            source_chunks = split_text(text=data, max_count=max_count, count_type=count_type, verbose=0)

            # Создание индексов документа
            search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings(), )

            count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), "cl100k_base")
            print('\n ===========================================: ')
            print('Количество токенов в документе :', count_token)
            print('ЦЕНА запроса:', 0.0004 * (count_token / 1000), ' $')
            return search_index

        def load_search_indexes(url: str, max_count, count_type) -> str:
            # Extract the document ID from the URL
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('Invalid Google Docs URL')
            doc_id = match_.group(1)

            # Download the document as plain text
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()
            text = response.text
            return create_embedding(text, max_count=max_count, count_type=count_type)

        def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):
            """Returns the number of tokens used by a list of messages."""
            try:
                encoding = tiktoken.encoding_for_model(model)
            except KeyError:
                encoding = tiktoken.get_encoding("cl100k_base")
            if model == "gpt-3.5-turbo-0301":  # note: future models may deviate from this
                num_tokens = 0
                for message in messages:
                    num_tokens += 4  # every message follows <im_start>{role/name}\n{content}<im_end>\n
                    for key, value in message.items():
                        num_tokens += len(encoding.encode(value))
                        if key == "name":  # if there's a name, the role is omitted
                            num_tokens += -1  # role is always required and always 1 token
                num_tokens += 2  # every reply is primed with <im_start>assistant
                return num_tokens
            else:
                raise NotImplementedError(
                    f"""num_tokens_from_messages() is not presently implemented for model {model}.""")

        def insert_newlines(text: str, max_len: int = 170) -> str:
            words = text.split()
            lines = []
            current_line = ""
            for word in words:
                if len(current_line + " " + word) > max_len:
                    lines.append(current_line)
                    current_line = ""
                current_line += " " + word
            lines.append(current_line)
            return "\n".join(lines)

        def answer_index(system, topic, search_index, temp=1, verbose=0, top_similar_documents=5):

            # Выборка документов по схожести с вопросом
            docs = search_index.similarity_search(topic, k=top_similar_documents)
            if (verbose): print('\n ===========================================: ')
            message_content = re.sub(r'\n{2}', ' ', '\n '.join(
                [f'\nОтрывок документа №{i + 1}\n=====================' + doc.page_content + '\n' for i, doc in
                 enumerate(docs)]))
            if (verbose): print('message_content :\n ======================================== \n', message_content)

            messages = [
                {"role": "system", "content": system + f"{message_content}"},
                {"role": "user", "content": topic}
            ]

            # example token count from the function defined above
            if (verbose): print('\n ===========================================: ')
            if (verbose): print(
                f"{num_tokens_from_messages(messages, 'gpt-3.5-turbo-0301')} токенов использовано на вопрос")

            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=temp
            )
            if (verbose): print('\n ===========================================: ')
            if (verbose): print(f'{completion["usage"]["total_tokens"]} токенов использовано всего (вопрос-ответ).')
            if (verbose): print('\n ===========================================: ')
            if (verbose): print('ЦЕНА запроса с ответом :', 0.002 * (completion["usage"]["total_tokens"] / 1000), ' $')
            if (verbose): print('\n ===========================================: ')
            print('ОТВЕТ : \n', insert_newlines(completion.choices[0].message.content))

            # return completion

        max_count = 110
        count_type = "words"
        top_similar_documents = 5

        # Документ "инструкция по эксплуатации"
        manual_index = load_search_indexes('', max_count=max_count,
                                           count_type=count_type)  # сюда добавьте ссылку на размеченную базу знаний

        manual_chat_promt = '''Ты консультант по использованию эхолота Fishin’ Buddy 100-серии Humminbird

        У тебя есть подробная информация по работе с ним.
        Тебе задает вопрос пользователь этого эхолота, дай ему информацию, опираясь на предоставленные материалы.
        Отвечай максимально точно и используй только информацию из документов, не добавляй ничего своего.
        Документ с информацией для ответа пользователю: '''

        ans = answer_index(
            manual_chat_promt,
            ' ',  # напишите вопрос для проверки инструкции
            manual_index,
            verbose=1,
            top_similar_documents=top_similar_documents
        )


if __name__ == '__main__':
    draft03_litetask().run("")