# -*- coding: utf-8 -*-

# !pip install tiktoken
# !pip install openai
# !pip install langchain
# !pip install faiss - cpu langchain openai tiktoken

class draft01_lesson():
    def __init__(self):
        pass

    def run(self, msg):
        """1 занятие | Основы дообучения. Роли ChatGPT. Подсчет токенов. LangChain. Ответы по регламенту | Дообучение ChatGPT | Гарант.ipynb

        Automatically generated by Colaboratory.

        Original file is located at
            https://colab.research.google.com/drive/abcdefgh

        # Обращение к ChatGPT по API. Роли. Влияние инструкции на ответ модели.

        Языковые модели, такие как GPT, создаются компанией OpenAI, и с каждой новой версией (от GPT-1 до последней, GPT-4), модель становится всё более мощной и точной. Это означает, что она может создавать более качественные и разнообразные тексты на основе предоставленного вами ввода.

        Если вы используете платную подписку **ChatGPT Plus**, у вас есть доступ к моделям GPT-3.5 и GPT-4. Это позволяет вам выбирать модель, которая наилучшим образом подходит для ваших нужд, в зависимости от конкретного применения. Например, GPT-4 может быть более полезен для более сложных задач, требующих более точного понимания и генерации текста.

        Через API OpenAI вы можете получить доступ к большему количеству моделей, хотя некоторые из них являются старыми версиями. Доступ к GPT-4 через API предоставляется только после одобрения OpenAI, поскольку это наиболее мощная модель и ее использование может потребовать большего контроля и ресурсов.

        Для большинства пользователей API наиболее подходящей и экономичной моделью будет gpt-3.5-turbo. Она предлагает оптимальное сочетание стоимости и производительности, что делает ее хорошим выбором для большинства приложений.

        Важно помнить о токенах при использовании этих моделей. Токены - это единицы текста, которые модель обрабатывает при каждом запросе. Количество токенов, используемых в каждом запросе, влияет на стоимость использования модели. Более длинные тексты потребуют больше токенов, и следовательно, будут стоить больше. Поэтому очень важно контролировать использование токенов, особенно при работе с большими объемами данных или при выполнении большого количества запросов. Это особенно важно при использовании GPT-4, поскольку эта модель может быстро исчерпать ваш бюджет. К подсчету токенов мы еще вернемся в данном занятии.

        Давайте посмотрим, как обращаться к GPT через API.

        Конечно же, для работы с GPT через API вам необходимо получить API ключ. Подробная инструкция, как подключиться к ChatGPT из России и оплатить использование GPT по API находится тут: https://docs.google.com/document/d/1Xnzx8Ve0gncUQREnnGMEa2IukNmYqZp0wD_3PTpWZSM/edit?usp=sharing
        """

        """Посмотрим, как можно взаимодействовать с моделью общения OpenAI (в данном случае, "gpt-3.5-turbo") через их API. Передадим сообщение в модель и попросим ее сгенерировать ответ.

        ChatGPT использует концепцию "ролей" для различных участников в диалоге.
        Каждое сообщение в списке messages является словарем, имеющим два поля: role и content. Role может принимать одно из трех значений: "system", "user" или "assistant", указывая, кто именно "говорит" сообщение. Content содержит само сообщение.

         В стандартном случае, три основные роли могут быть включены в чат:

        * **"system"**: Системная роль используется для установки контекста диалога. Это как общая инструкция для модели, которая определяет её поведение во время диалога. Сообщения от "system" обычно помещаются в начале диалога и могут содержать информацию, как модель должна вести себя в роли ассистента.

        * **"user"**: Это роль для человека- пользователя, который ведет диалог с моделью. Ваши запросы к модели будут выставлены под этой ролью.

        * **"assistant"**: Это роль для модели AI, которая отвечает на запросы от пользователя.

        При создании чата с моделью, вы предоставляете список сообщений. Каждое сообщение в этом списке — это словарь, содержащий два элемента: "role" и "content". "Role" указывает на роль (вышеупомянутые "system", "user" или "assistant"), а "content" содержит фактический текст сообщения.
        """

        import getpass
        import openai
        import os
        # Получение ключа API от пользователя и установка его как переменной окружения
        openai_key = getpass.getpass("OpenAI API Key:")
        os.environ["OPENAI_API_KEY"] = openai_key
        openai.api_key = openai_key

        # Теперь вы можете взаимодействовать с API OpenAI
        result = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Ты старательный помощник."},
                {"role": "user", "content": "Кто является чебурашкой"}
            ]
        )

        print(result)  # вывод результата запроса
        message = result['choices'][0]['message']['content']
        print(message)

        """Ответ (result) возвращается в формате JSON, и Python выводит Unicode символы в своем экранированном представлении. Это не ошибка, а стандартное поведение Python при работе с Unicode.

        Чтобы получить ответ на русском языке, вы можете извлечь содержимое ответа из JSON и распечатать его (как это сделано в примере выше).
        Давайте попробуем поменять содержание "system" и посмотрим, как это изменит суть ответа:
        """

        result = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system",
                 "content": "Ты -старый ворчун, недовольный и грубый, отвечай по-хамски и с недовольством, груби, оскорбляй"},
                {"role": "user", "content": "Кто является чебурашкой"}
            ]
        )

        print(result)  # вывод результата запроса
        message = result['choices'][0]['message']['content']
        print(message)

        """"system": это инструкции для ассистента о том, как он должен вести себя. В этом случае, "Ты -старый ворчун, недовольный и грубый, отвечай по-хамски и с недовольством, груби, оскорбляй" указывает модели вести себя как старый ворчун. Системные сообщения помогают задать "личность" и "миссию" модели на протяжении разговора.
        "user": это входные данные от пользователя, на которые модель должна ответить. В данном случае, вопросы пользователя: "Кто является чебурашкой?"

        Поскольку модель не имеет доступа к реальному времени или обновлениям в интернете после ее последнего обновления, ответы модели основаны только на данных, на которых она была обучена, и на предоставленном контексте общения. То есть, если завтра поменяется чебурашка, модель об этом не будет знать и будет отвечать устаревшей информацией.
        """

        result = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Ты - специалист, который отвечает на старо-русском языке"},
                {"role": "user", "content": "Кто является чебурашкой"}
            ],
            n=2
        )

        print(result)  # вывод результата запроса
        message = result['choices'][0]['message']['content']
        print(message)

        for i, choice in enumerate(result['choices']):
            message = choice['message']['content']
            print(f"Answer {i + 1}: {message}")

        """Думаю, вы обратили внимание на поле **usage**: Это поле содержит информацию о том, сколько токенов было использовано при выполнении запроса:
        * *prompt_tokens*: Количество токенов, использованных во входных данных (то есть в ваших сообщениях).
        * *completion_tokens*: Количество токенов, которые были сгенерированы в качестве ответа моделью.
        * *total_tokens*: Общее количество токенов, использованных в этом запросе. Это число равно сумме prompt_tokens и completion_tokens.

        **choices**: Список сгенерированных ответов модели. В большинстве случаев здесь будет только один ответ, но если вы используете параметр "n" (который контролирует количество ответов, которые должна сгенерировать модель), то здесь может быть больше одного элемента.

        **finish_reason**: Причина, по которой модель прекратила генерацию ответа. Значение "stop" означает, что модель определила, что ее ответ полон и закончен. Значение "length" означает, что модель достигла максимального количества токенов и прекратила генерацию ответа.

        **index:** Индекс сгенерированного ответа в массиве "choices". Если вы запрашиваете только один ответ, то его индекс будет всегда равен 0. Если вы запрашиваете несколько ответов, то каждый из них будет иметь свой уникальный индекс, начинающийся с 0.

        **max_tokens**: определяет максимальное количество токенов, которое модель генерирует за одну итерацию. Токен в данном контексте обычно соответствует слову или части слова. Ограничение max_tokens помогает контролировать длину генерируемого текста моделью.  Если вы укажете слишком маленькое число, то ответ модели может быть обрезан и иметь неполный смысл.

        **temperature**: управляет степенью случайности выходных данных модели. Значения близкие к нулю делают вывод более определенным и консервативным, с меньшим разнообразием слов, но более точным и предсказуемым. Более высокие значения (ближе к 1) приводят к более случайному и разнообразному выводу. В некотором смысле temperature определяет "креативность" модели: при более высоких значениях модель может генерировать более необычные и неожиданные ответы, но при этом риск ошибки увеличивается.

        **n**: Этот параметр контролирует количество независимых ответов, которые должна сгенерировать модель на ваш вход.
        """

        result = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Ты -мечтатель и поэт, отвечаешь только стихами"},
                {"role": "user", "content": "При каких обстоятельствах был убит Пушкин?"}
            ]
        )

        print(result)  # вывод результата запроса
        message = result['choices'][0]['message']['content']
        print(message)

        """А теперь используем параметры temperature и max_tokens:

        """

        result = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Ты -мечтатель и поэт, отвечаешь только стихами"},
                {"role": "user", "content": "При каких обстоятельствах был убит Пушкин?"}
            ],
            temperature=0,
            max_tokens=50
        )

        print(result)  # вывод результата запроса
        message = result['choices'][0]['message']['content']
        print(message)

        """Таким образом, благодаря параметрам temperature и max_tokens мы можем управлять ответом модели, сделав его более определенным или "случайным", а также длиннее или короче.

        # Подсчет токенов

        Токены представляют собой основную единицу работы для моделей GPT. Вместо работы с целыми словами, модель разбивает текст на меньшие части, которые называются токенами. Это могут быть отдельные символы, сложные слова или даже целые слова, в зависимости от языка и контекста. Для английского языка большинство слов занимают от одного до двух токенов. Например, слово "ChatGPT" занимает два токена: "Chat" и "GPT". Для русского языка ситуация другая - 1 токен не равен 1 слову, а, скорее, 1 токен - 1 слог.

        Ограничение по токенам связано с максимальным количеством токенов, которое модель может обработать за один раз. Для gpt-3.5-turbo это ограничение составляет 4096 токенов. Это означает, что сумма токенов в ваших входных сообщениях и ответах модели не может превышать 4096.

        Недавно появилась модель gpt-3.5-turbo-16k. Эта модель предлагает в четыре раза большую длину контекста, чем базовая модель 4k.

        Поскольку использование токенов влияет на стоимость и время выполнения запроса, важно учитывать количество токенов в ваших запросах. Ваш запрос может быть отклонен, если он превышает максимальное ограничение по токенам. Кроме того, если входные данные содержат слишком много токенов, то может оказаться недостаточно токенов для получения полезного ответа от модели. Поэтому иногда может потребоваться управлять количеством токенов в ваших входных данных.

        Когда речь идет о коротких запросах, типа тех, которые мы делали выше, проблем не возникает, мы не превышаем лимит по токенам. Тогда, когда для наших целей мы должны подавать в модель большие отрезки текста, подсчет токенов становится крайне важной темой.
        Давайте разберемся, как можно просто подсчитать количество токенов в подаваемых текстах:

        Для начала, создадим переменные, в которых будут находится тексты, которые мы будем подавать в system и user:
        """

        system = '''Ты менеджер поддержки в чате компании, компания продает курсы по AI. У тебя есть большой документ со всеми материалами о продуктах компании.
        Тебе задает вопрос клиент в чате, дай ему ответ, опираясь на документ, постарайся ответить так, чтобы человек захотел после ответа купить обучение.
        Отвечай максимально точно по документу, не придумывай ничего от себя. Не упоминай документ при ответе, клиент ничего не должен знать о документе, по которому ты отвечаешь.'''

        base_info = '''Курс по AutoML состоит из пяти занятий, каждое из которых посвящено конкретным темам:Создание моделей на AutoKeras.
        На этом занятии будет полный разбор фреймворка AutoKeras, включая все Подробно разберем всё на AutoKeras, все гриды, тюнеры и другие особенности.
        Оптимизация моделей на KerasTuner и Talos. Методы подбора параметров сетки для заданной архитектуры модели на фреймворках.
        Библиотеки для построения генетических алгоритмов. Аналог AutoKeras. Как пользоваться и как в целом устроен.
        Оптимизация сложных моделей на KerasTuner и Talos. Углубление в тему оптимизации архитектур, которые AutoKeras  не может создавать, на фреймворках KerasTuner и Talos.
        Будут рассмотрены примеры, включая построение U-Net для сегментации изображений.
        Свободная тема на выбор группы. Студенты сами выберут тему из предложенных на курсе. Например разбор генетических алгоритмов, Auto-sklearn или Light AutoML, или их комбинацию.
        Как получить курс:
        Курс не доступен для покупки, а предоставляется в качестве подарка новичкам, приобретающим любой тариф, кроме базового.
        Также курс доступен и для старичков, которые могут получить его в подарок при любой покупке на сумму 40 000 рублей.
        К примеру, это может быть покупка курса по PyTorch или Интеграции в Production, стажировок на год, либо повышение тарифа с базового на основной или продвинутый.'''

        question = "Сколько занятий в курсе по AutoML"

        import tiktoken

        def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):
            """Возвращает количество токенов, используемых списком сообщений."""
            try:
                encoding = tiktoken.encoding_for_model(model)  # Пытаемся получить кодировку для выбранной модели
            except KeyError:
                encoding = tiktoken.get_encoding(
                    "cl100k_base")  # если не получается, используем кодировку "cl100k_base"
            if model == "gpt-3.5-turbo-0301" or "gpt-3.5-turbo-0613" or "gpt-3.5-turbo-16k" or "gpt-3.5-turbo":
                num_tokens = 0  # начальное значение счетчика токенов
                for message in messages:  # Проходимся по каждому сообщению в списке сообщений
                    num_tokens += 4  # каждое сообщение следует за <im_start>{role/name}\n{content}<im_end>\n, что равно 4 токенам
                    for key, value in message.items():  # итерация по элементам сообщения (роль, имя, контент)
                        num_tokens += len(encoding.encode(value))  # подсчет токенов в каждом элементе
                        if key == "name":  # если присутствует имя, роль опускается
                            num_tokens += -1  # роль всегда требуется и всегда занимает 1 токен, так что мы вычитаем его, если имя присутствует
                num_tokens += 2  # каждый ответ начинается с <im_start>assistant, что добавляет еще 2 токена
                return num_tokens  # возвращаем общее количество токенов
            else:
                # Если выбранная модель не поддерживается, генерируем исключение
                raise NotImplementedError(
                    f"""num_tokens_from_messages() is not presently implemented for model {model}. # вызываем ошибку, если функция не реализована для конкретной модели""")

        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": base_info + f"{question}"}
        ]

        print(f"{num_tokens_from_messages(messages, 'gpt-3.5-turbo-0301')} токенов использовано на вопрос")

        completion = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0
        )
        print(completion)
        message = completion['choices'][0]['message']['content']
        print("Ответ ChatGPT", message)

        """То есть, в функцию num_tokens_from_message мы подаем инструкцию для модели, а также базовую информацию, по которой модель должна ответить и непосредственно вопрос пользователя, и получаем количество токенов.
        Это количество можно сравнить с тем, что ChatGPT указывает в своих подсчетах - в "prompt_tokens". В данном случае, погрешность всего в 1 токен.

        Можно подсчитать токены и другим способом:
        """


        from langchain.chat_models import ChatOpenAI
        from langchain.schema import (
            AIMessage,
            HumanMessage,
            SystemMessage
        )

        messages = [
            SystemMessage(content=system),
            HumanMessage(content=base_info + f"{question}")
        ]

        chat = ChatOpenAI(temperature=0)
        chat.get_num_tokens_from_messages(messages)

        """# Деление текста на чанки. Создание индексной базы. Langchain.

        Однако обычно база знаний не ограничивается парой абзацев текста, а содержит сотни страниц информации, размеры которой будут явно превышать 4096 токенов. То есть, нам нужно, чтобы из большого объема информации вычленялись отдельные отрезки текста, относящиеся по смыслу к конкретному вопросу, на который будет отвечать ChatGPT. И тут нам на помощь приходит LangChain и чанки. Но давайте обо всем по порядку:

        Чанки очень полезны при обработке больших объемов текста. Они позволяют разбить текст на управляемые части и обрабатывать эти части по мере необходимости. Это особенно полезно, так как у нас ограничены ресурсы, такие как количество токенов, которые может обработать ChatGPT, и мы не можем подавать в нее весь текст за один раз.

        **CharacterTextSplitter** - это один из инструментов, который разбивает входной текст на чанки (или куски) определенного размера, разделенные указанным разделителем.

        CharacterTextSplitter имеет следующие параметры:

        * *separator* - это символ или строка, используемые для разделения текста на чанки. Например, может использоваться символ новой строки ("\n").

        * *chunk_size* - это максимальное количество символов, которые могут быть в каждом чанке.

        * *chunk_overlap* - это количество символов, которые будут перекрываться между соседними чанками. Например, если указать значение 0, это означает, что нет перекрытия между чанками.

        То есть, CharacterTextSplitter принимает текст и разбивает его на чанки с учетом указанных параметров. Давайте посмотрим, как он работает:
        """

        """Импортируем необходимые библиотеки из фреймворка"""

        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.text_splitter import CharacterTextSplitter
        from langchain.vectorstores import FAISS
        from langchain.document_loaders import TextLoader
        import os
        import getpass

        os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")

        """Загрузим текст из файла База знаний.txt:
        https://docs.google.com/document/d/abcdefgh/edit?usp=sharing

        Разделим на отрезки(chunks)
        """

        loader = TextLoader("/content/База знаний Гарант.txt")

        documents = loader.load()
        text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=0)
        docs = text_splitter.split_documents(documents)

        # всего получилось чанков:
        len(docs)

        # первый чанк
        print(docs[0])
        page_content = docs[0].page_content
        # длина первого чанка
        print(len(page_content))

        # Устанавливаем метаданные для первого чанка
        docs[0].metadata = {"teg": "Описание Гарант"}

        # выводим на печать обновленные метаданные чанка
        print(docs[0].metadata)

        # второй чанк:
        docs[1]

        """В выводе ячейки видно что с указанным разделителем (\n - перенос строки) у нас есть несколько предупреждений о превышении количества установленных токенов. Здесь в дальнейшем потребуется установить либо другой разделитель либо использовать другой сплитер во избежания переполнения контеста, который мы будем подавать в модель.

        Помимо **CharacterTextSplitter**, который просто разбивает текст на части по указанному размеру символов. Например, если размер чанка составляет 500 символов, он разделит текст на куски определенного размера - количества символов каждый, есть еще и другие разделители (или "сплиттеры") предназначены для разделения больших блоков текста на меньшие фрагменты или "чанки". Основное различие между ними заключается в способе, которым они разделяют текст.

        * **RecursiveCharacterTextSplitter** делает то же самое, что и CharacterTextSplitter, но если текст не может быть равномерно разделен (например, текст длиной 1500 символов и размер чанка 500), он будет рекурсивно делить последний чанк, пока все части не станут одного размера. Это может быть полезно, если текст содержит очень длинные отрезки без разделителей, таких как пробелы или переносы строки.

        * **NLTKTextSplitter** использует библиотеку NLTK (Natural Language Toolkit) для разделения текста на предложения. Так, если размер чанка составляет 5, каждый чанк будет содержать 5 предложений. Это может быть полезно, если важно сохранять целые предложения. *chunk_size* и *chunk_overlap* здесь интерпретируются как количество предложений, а не количество символов.

        * **TokenTextSplitter** разделяет текст на чанки по указанному количеству токенов. chunk_size - это количество токенов в каждом фрагменте, а chunk_overlap - это количество токенов, которые перекрываются между фрагментами.

        Напомним, что эти разделители удобны при работе с большими объемами текста. Они позволяют разбивать большой текст на меньшие части, которые затем можно обрабатывать по отдельности. Выбор разделителя текста зависит от конкретной задачи и данных. Если важно сохранить контекст предложения или абзаца, может быть полезно использовать NLTKTextSplitter или TokenTextSplitter. Если контекст не так важен, или если текст очень длинный и его нужно разбить на меньшие части, CharacterTextSplitter или RecursiveCharacterTextSplitter могут быть более подходящими.

        ## Перевод в эмбеддинги и FAISS

        Теперь переведем каждый отрезкок (чанк) в вектор с помощью модели Ada v2 от OpenAI, и создадим индексную базу db

        Мы рассмотрим пример использования векторного хранилища FAISS из фреймворка langchain для поиска по отрывкам текста (chunks) контекста схожим по L2 расcтоянию (Евклидовому растоянию)

        Facebook AI Similarity Search (Faiss) - это библиотека для эффективного поиска сходства и кластеризации плотных векторов. Он содержит алгоритмы, которые выполняют поиск в наборах векторов любого размера, вплоть до тех, которые, возможно, не помещаются в оперативную память. Он также содержит вспомогательный код для оценки и настройки параметров. И что не мало важно является бесплатным!
        """

        # Инициализирум модель эмбеддингов
        embeddings = OpenAIEmbeddings()

        # Создадим индексную базу из разделенных фрагментов текста
        db = FAISS.from_documents(docs, embeddings)

        """# Поиск текста по схожести

        **similarity_search** это метод, который используется для поиска наиболее похожих документов на заданную тему или запрос.

        При вызове этого метода, он принимает тему или запрос в качестве входных данных и число k, которое означает количество документов, которые нужно вернуть. Затем он вычисляет векторное представление темы или запроса (часто называемое "эмбеддингом") с использованием вашей модели эмбеддингов.

        Этот вектор затем сравнивается со всеми векторами в вашем индексе FAISS (который предварительно создается из всех документов в вашей базе данных) с использованием метрики похожести (Евклидово расстояние). Евклидово расстояние часто используется для измерения сходства между двумя векторами или точками данных. Близкие точки (с малым Евклидовым расстоянием) считаются более похожими, чем дальние точки (с большим Евклидовым расстоянием).

        Метод similarity_search затем возвращает k документов, которые имеют наибольшее сходство с запросом. Эти документы могут затем использоваться для ответа на запрос или для других целей.

        Пример поиска (метод similarity_search индексной базы) отрывков текста по схожести из документа по нашему запросу. k-это количество фрагментов которые отобразятся в результате поиска (с наименьшим растоянием по тесту запроса)
        """

        query = "нужно ли знать математику?"
        docs = db.similarity_search(query, k=4)

        print("**отрезок №1**", docs[0].page_content)
        print("**отрезок №2**", docs[1].page_content)
        print("**отрезок №3**", docs[2].page_content)
        print("**отрезок №4**", docs[3].page_content)

        query = "сколько времени в неделю нужно тратить на обучение?"
        docs = db.similarity_search(query, k=4)

        print("**отрезок №1**", docs[0].page_content)
        print("**отрезок №2**", docs[1].page_content)
        print("**отрезок №3**", docs[2].page_content)
        print("**отрезок №4**", docs[3].page_content)

        """## Метод similarity_search_with_score

        Существует несколько специфических методов FAISS. Одним из них является similarity_search_with_score, который позволяет возвращать не только документы, но и оценку расстояния запроса к ним. Возвращаемая оценка расстояния равна расстоянию L2. Следовательно, чем ниже оценка, тем лучше.
        """

        docs_and_scores = db.similarity_search_with_score(query, k=4)

        # первый фрагмент
        print("Контент: ", docs_and_scores[0][0].page_content)
        print("Оценка расстояния: ", docs_and_scores[0][1])

        # второй фрагмент
        print("Контент: ", docs_and_scores[1][0].page_content)
        print("Оценка расстояния: ", docs_and_scores[1][1])

        """Как мы можем видеть что между значениями расстояний этих двух отрывков разница около 0,01, и действительно, в обоих этих отрезках документов речь идет именно об определении времени на учебу.

        Чтобы при каждом запуске не пересоздавать индексированную базу, сохраним ее на Гугл Драйв:
        """

        from google.colab import drive
        drive.mount('/content/drive')
        db.save_local("/content/drive/My Drive/faiss_index")

        """# Простая задача - ответ по базе знаний компании

        Давайте попробуем дообучить ChatGPT на базе знаний университета искусственного интеллекта, чтобы можно было получать ответы на вопросы по информации, содержащейся в этой базе знаний.
        """

        import re
        import requests
        import openai
        from langchain.docstore.document import Document

        def load_document_text(url: str) -> str:
            # функция для загрузки документа по ссылке из гугл док
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('Invalid Google Docs URL')
            doc_id = match_.group(1)
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()
            text = response.text

            return text

        def answer_index(system, topic, search_index, temp=0, verbose=0):

            # Поиск релевантных отрезков из базы знаний
            docs = search_index.similarity_search(topic, k=4)
            if verbose: print('\n ===========================================: ')
            message_content = re.sub(r'\n{2}', ' ', '\n '.join(
                [f'\nОтрывок документа №{i + 1}\n=====================' + doc.page_content + '\n' for i, doc in
                 enumerate(docs)]))
            if verbose: print('message_content :\n ======================================== \n', message_content)

            messages = [
                {"role": "system", "content": system},
                {"role": "user",
                 "content": f"Документ с информацией для ответа клиенту: {message_content}\n\nВопрос клиента: \n{topic}"}
            ]

            if verbose: print('\n ===========================================: ')

            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=temp
            )
            if verbose: print('\n ===========================================: ')
            answer = completion.choices[0].message.content
            return answer  # возвращает ответ

        # Инструкция для GPT, которая будет подаваться в system
        system = load_document_text('https://docs.google.com/document/d/abcdefgh')

        temperature = 0
        verbose = 0

        # вопрос пользователя
        topic = "Смогу ли я освоить программу, если я не умею программировать?"

        ans = answer_index(system, topic, db, temp=1, verbose=1)
        ans

        # вопрос пользователя
        topic = "Необходимо ли знать математику?"
        ans = answer_index(system, topic, db, temp=1, verbose=0)
        ans

        # вопрос пользователя
        topic = "смогу ли я после обучения сразу найти работу по новой специальности?"
        ans = answer_index(system, topic, db, temp=1, verbose=0)
        ans

if __name__ == '__main__':
    draft01_lesson().run("")