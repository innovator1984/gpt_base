# -*- coding: utf-8 -*-

# !pip install openai
# !pip install faiss - cpu langchain openai tiktoken

class draft02_intro():
    def __init__(self):
        pass

    def run(self, msg):
        """2 занятие | Принципы написания эффективных инструкций. Создание диалога. Саммаризация. Задачи с несколькими моделями. Нейро-контроль качества | Дообучение ChatGPT | Гарант.ipynb

        Automatically generated by Colaboratory.

        Original file is located at
            https://colab.research.google.com/drive/abcdefgh

        # Принципы написания эффективных инструкций (промптов) при обращении к ChatGPT через API

        При создании промптов (инструкций) для модели ChatGPT есть несколько принципов, которые могут помочь вам получить лучшие результаты:

        * **Конкретизация**: Чем конкретнее ваш промпт, тем более конкретный ответ вы получите. Ваши инструкции должны быть понятными и конкретными. Если инструкции слишком общие или неоднозначные, модель может не понять, что вы от нее хотите.

        * **Системное сообщение**: Использование системного сообщения (system) для указания поведения модели может быть очень полезным. Например, вы можете указать в системном сообщении: "Вы - помощник, который специализируется на истории искусства и всегда стремится предоставить подробные и точные ответы". Это сообщение задает контекст для модели. Системное сообщение позволяет установить задачу или дать инструкции модели. Это сообщение видит модель, но оно не считается частью диалога с пользователем.

        * **Инструкции в сообщении пользователя**: Краткую нструкцию можно также поместить и в user. Например: "Ответь на вопрос клиента согласно представленной тебе информации".

        * **Контроль длины ответа**: В тексте инструкции можно контролировать длину ответа модели.

        * **few-shot** подход -это подача в модель нескольких примеров входных и выходных данных перед актуальным запросом. Это помогает "указать" модели на конкретную задачу, которую нужно выполнить. Few-shot подход позволяет достигать более точных результатов на новых или специфических задачах, поскольку он помогает модели понять контекст и тип задачи, основываясь на предоставленных примерах. Однако точность результата в значительной степени зависит от качества и релевантности предоставленных примеров.

        * **Тестирование и итерация**: Проведите несколько тестовых запусков с разными промптами и параметрами, чтобы увидеть, какой из них лучше всего работает для вашего конкретного случая.

        Это общие рекомендации и они могут не всегда работать в каждом конкретном случае, поэтому экспериментирование и итерация - важная часть процесса. Помните, что эффективность промпта часто зависит от специфики задачи, поэтому возможно потребуется время на эксперименты и тестирование различных подходов.

        Примером, показывающим преимущества few-shot подхода, может быть сценарий, когда вы хотите использовать AI для создания стилизованных описаний продуктов для интернет-магазина. Важно, чтобы AI понимал специфический стиль и тон, которые вы хотите использовать.

        Возьмем, например, задачу создания описаний для винтажной мебели. В этом случае использование few-shot подхода может дать следующие результаты:
        """

        import openai
        import getpass
        import os
        # Получение ключа API от пользователя и установка его как переменной окружения
        openai_key = getpass.getpass("OpenAI API Key:")
        os.environ["OPENAI_API_KEY"] = openai_key
        openai.api_key = openai_key

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system",
                 "content": "Вы - AI-ассистент, который специализируется на создании описаний винтажной мебели в романтическом и мечтательном стиле."},
                {"role": "user", "content": "Опишите старинный деревянный стол."},
                {"role": "assistant",
                 "content": "Этот деревянный стол, как будто воплощает в себе дух прошлых времен. Его старинные, но удивительно прочные деревянные ножки, навевают образы бесконечных обедов на открытом воздухе и ночных разговоров при свечах. С каждым следом износа и каждой потертостью на его поверхности, он рассказывает историю своей долгой и богатой жизни."},
                {"role": "user", "content": "Опишите винтажное зеркало в золотой раме."},
            ]
        )

        print(response['choices'][0]['message']['content'])

        """Даже если из инструкции уберем указания на романтический и мечтательный стиль, модель, получив примеры ответов, будет им следовать."""

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Вы - ассистент, описывающий винтажную мебель"},
                {"role": "user", "content": "Опишите старинный деревянный стол."},
                {"role": "assistant",
                 "content": "Этот деревянный стол, как будто воплощает в себе дух прошлых времен. Его старинные, но удивительно прочные деревянные ножки, навевают образы бесконечных обедов на открытом воздухе и ночных разговоров при свечах. С каждым следом износа и каждой потертостью на его поверхности, он рассказывает историю своей долгой и богатой жизни."},
                {"role": "user", "content": "Опишите старинное кресло"},
            ]
        )

        print(response['choices'][0]['message']['content'])

        """# Создание диалога. Саммаризация.

        Для некоторых задач требуется возможность создания и накапливания диалога для дальнейшей его саммаризации и передачи в chatGPT вместе со следующим запросом пользователя, базой знаний и инструкцией. Вот, как можно это реализовать
        """

        # импортируем необходимые библиотеки
        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.text_splitter import CharacterTextSplitter
        from langchain.vectorstores import FAISS
        from langchain.document_loaders import TextLoader
        import os
        import getpass
        import re
        import requests
        import openai
        from langchain.docstore.document import Document
        import logging
        logging.getLogger("langchain.text_splitter").setLevel(logging.ERROR)
        logging.getLogger("chromadb").setLevel(logging.ERROR)

        # Получение ключа API от пользователя и установка его как переменной окружения
        openai_key = getpass.getpass("OpenAI API Key:")
        os.environ["OPENAI_API_KEY"] = openai_key
        openai.api_key = openai_key

        # функция для загрузки документа по ссылке из гугл драйв
        def load_document_text(url: str) -> str:
            # Extract the document ID from the URL
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('Invalid Google Docs URL')
            doc_id = match_.group(1)

            # Download the document as plain text
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()
            text = response.text

            return text

        # Инструкция для GPT, которая будет подаваться в system
        system = load_document_text('https://docs.google.com/document/d/abcdefgh')

        # База знаний, которая будет подаваться в langChain
        database = load_document_text('https://docs.google.com/document/d/hgfedcba')

        # делим текст на чанки и создаем индексную базу
        source_chunks = []
        splitter = CharacterTextSplitter(separator="\n", chunk_size=1024, chunk_overlap=0)

        for chunk in splitter.split_text(database):
            source_chunks.append(Document(page_content=chunk, metadata={}))

        # Инициализирум модель эмбеддингов
        embeddings = OpenAIEmbeddings()

        # Создадим индексную базу из разделенных фрагментов текста
        db = FAISS.from_documents(source_chunks, embeddings)

        # Функция, которая позволяет выводить ответ модели в удобочитаемом виде
        def insert_newlines(text: str, max_len: int = 170) -> str:
            words = text.split()
            lines = []
            current_line = ""
            for word in words:
                if len(current_line + " " + word) > max_len:
                    lines.append(current_line)
                    current_line = ""
                current_line += " " + word
            lines.append(current_line)
            return "\n".join(lines)

        # функция для получения ответа от модели
        def answer_index(system, topic, search_index, temp=1, verbose=0):

            # Поиск релевантных отрезков из базы знаний
            docs = search_index.similarity_search(topic, k=4)
            if verbose: print('\n ===========================================: ')
            message_content = re.sub(r'\n{2}', ' ', '\n '.join(
                [f'\nОтрывок документа №{i + 1}\n=====================' + doc.page_content + '\n' for i, doc in
                 enumerate(docs)]))
            if verbose: print('message_content :\n ======================================== \n', message_content)

            messages = [
                {"role": "system", "content": system},
                {"role": "user",
                 "content": f"Документ с информацией для ответа клиенту: {message_content}\n\nВопрос клиента: \n{topic}"}
            ]

            if verbose: print('\n ===========================================: ')

            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo-16k",
                messages=messages,
                temperature=temp
            )
            answer = insert_newlines(completion.choices[0].message.content)
            return answer  # возвращает ответ

        """Делаем функцию, которая будет саммаризировать диалог по мере его накапливания, и данную саммаризацию мы будем подавать модели, которая отвечает на вопрос клиента, чтобы модель учитывала контекст диалога."""

        def summarize_questions(dialog):
            # Применяем модель gpt-3.5-turbo-0613 для саммаризации вопросов
            messages = [
                {"role": "system",
                 "content": "Ты - ассистент отдела продаж, основанный на AI. Ты умеешь профессионально суммаризировать присланные тебе диалоги менеджера и клиента. Твоя задача - суммаризировать диалог, который тебе пришел."},
                {"role": "user",
                 "content": "Суммаризируй следующий диалог менеджера по продажам и клиента: " + " ".join(dialog)}
            ]

            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo-0613",
                messages=messages,
                temperature=0.3,  # Используем более низкую температуру для более определенной суммаризации
                max_tokens=1000  # Ограничиваем количество токенов для суммаризации
            )

            return completion.choices[0].message.content

        """Далее следует основная функция, объединяющая все предыдущие. В нее мы подаем инструкцию, базу знаний, текущий вопрос клиента из чата, а также историю предыдущего диалога - при наличии."""

        def answer_user_question_dialog(system: str, db: str, user_question: str, question_history: list) -> str:

            # Если в истории более одного вопроса, применяем суммаризацию
            summarized_history = ""
            if len(question_history) > 0:
                summarized_history = "Вот краткий обзор предыдущего диалога: " + summarize_questions(
                    [q + ' ' + (a if a is not None else '') for q, a in question_history])

            # Добавляем явное разделение между историей диалога и текущим вопросом
            input_text = summarized_history + "\n\nТекущий вопрос: " + user_question

            # Извлечение наиболее похожих отрезков текста из базы знаний и получение ответа модели
            answer_text = answer_index(system, input_text, db)

            # Добавляем вопрос пользователя и ответ системы в историю
            question_history.append((user_question, answer_text if answer_text is not None else ''))

            # Выводим суммаризированный текст, который видит модель
            if summarized_history != "":
                print('****************************')
                print(insert_newlines(summarized_history))
                print('****************************')

            return insert_newlines(answer_text)

        """Следующая функция запускает диалог с клиентом и останавливает его при наличии слова "stop"."""

        def run_dialog(system_doc_url, knowledge_base_url):
            question_history = []
            dialog = ""
            while True:
                user_question = input('Клиент: ')
                if user_question.lower() == 'stop':
                    break
                answer = answer_user_question_dialog(system_doc_url, knowledge_base_url, user_question,
                                                     question_history)
                dialog += f'\nКлиент: {user_question} \n Менеджер: {answer}'
                print('\nМенеджер: ', answer)

            return

        temperature = 0.5
        verbose = 0

        run_dialog(system, db)

if __name__ == '__main__':
    draft02_intro().run("")