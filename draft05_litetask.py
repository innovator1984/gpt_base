# -*- coding: utf-8 -*-
"""
        !pip
        install
        openai
        langchain
        chromadb
        faiss - cpu
        tiktoken
        pydantic == 1.10
        .8
"""
class draft05_litetask():
    def __init__(self):
        pass

    def run(self, msg):
        # -*- coding: utf-8 -*-
        """CHAT00_dz00_Вебинар 25 июля. Новые фишки chatGPT

        Automatically generated by Colaboratory.

        Original file is located at
            https://colab.research.google.com/drive/1pfkmz4HNy1kO58SeJ6Dy0GvVNgFU0xFo

        ## Установка зависимостей
        """


        """## Авторизация в OpenAI"""

        # @title Введите OpenAI токен
        import os
        import getpass

        os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")

        """## 1. Динамический выбор из нескольких приглашений (промтов)

        Создание цепочки, которая динамически выбирает подсказку для использования при заданном вводе. В частности, мы покажем, как использовать MultiPromptChain для создания цепочки вопросов-ответов, которая выбирает приглашение, наиболее релевантное для данного вопроса, а затем отвечает на вопрос, используя это приглашение.
        https://python.langchain.com/docs/modules/chains/additional/multi_prompt_router
        """

        # @title Сервисные функции
        from langchain.chains.router import MultiPromptChain
        from langchain.chat_models import ChatOpenAI

        # Отключение трасировки предупреждающих сообщений при выполнении кода
        os.environ["LANGCHAIN_TRACING"] = "false"

        # Подготавливаем темплейты из которых будет осуществлен выбор по описанию для наших задач

        physics_template = """Вы очень умный профессор физики. \
        У вас отлично получается отвечать на вопросы по физике в краткой и понятной форме. \
        Когда вы не знаете ответа на вопрос, вы признаете, что не знаете.

        Вот такой вопрос:
        {input}"""

        math_template = """Вы очень хороший математик. Ты отлично отвечаешь на математические вопросы. \
        Вы так хороши, потому что умеете разбивать сложные проблемы на составные части,
        отвечать на составные части, а затем сводить их воедино, чтобы ответить на более широкий вопрос.

        Вот такой вопрос:
        {input}"""

        # Добавляем информацию и описание для промтов

        prompt_infos = [
            {
                "name": "physics",
                "description": "Хорошо подходит для ответов на вопросы по физике",
                "prompt_template": physics_template
            },
            {
                "name": "math",
                "description": "Хорошо подходит для ответов на математические вопросы",
                "prompt_template": math_template
            }
        ]

        chain = MultiPromptChain.from_prompts(ChatOpenAI(), prompt_infos, verbose=True)

        # Запускаем на вопросе из раздела физики (в промежуточных итогах видим что выбран промт physics)

        print(chain.run("Что такое излучение черного тела?"))

        # Запускаем на вопросе из раздела математики (в промежуточных итогах видим что выбран промт math)

        print(chain.run("Каково первое простое число, большее 40, такое, что единица плюс простое число делится на 3"))

        """## 2. Динамический выбор из нескольких баз знаний - "ретриверов" (векторных хранилищ)
        Создание цепочки, которая динамически выбирает, какую поисковую систему использовать. В частности, мы покажем, как использовать MultiRetrievalQAChain для создания цепочки вопросов-ответов, которая выбирает цепочку поиска QA (вопрос-ответ), наиболее подходящую для данного вопроса, а затем отвечает на вопрос, используя ее.
        https://python.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router
        """

        # @title Сервисные функции
        from langchain.chains.router import MultiRetrievalQAChain
        from langchain.chat_models import ChatOpenAI
        from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
        from langchain.embeddings import OpenAIEmbeddings
        from langchain.document_loaders import TextLoader
        from langchain.vectorstores import FAISS

        # Отключение трасировки предупреждающих сообщений при выполнении кода
        os.environ["LANGCHAIN_TRACING"] = "false"

        """
        !wget - O
        UII_history_120623.txt
        "https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/fEwN40_ieKi_0g"
        !wget - O
        UII_learning_120623.txt
        "https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/v4sae3Lad4Yn-Q"
        """
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        history_docs = TextLoader('/content/UII_history_120623.txt').load()
        history_documents = text_splitter.split_documents(history_docs)
        history_retriever = FAISS.from_documents(history_documents, OpenAIEmbeddings()).as_retriever()

        learning_docs = TextLoader('/content/UII_learning_120623.txt').load()
        learning_documents = text_splitter.split_documents(learning_docs)
        learning_retriever = FAISS.from_documents(learning_documents, OpenAIEmbeddings()).as_retriever()

        personal_texts = [
            "Я люблю яблочный пирог (шарлотку)",
            "Мой любимый цвет фуксии",
            "Моя мечта стать проффесииональным танцором",
            "Я сломал руку, когда мне было 12",
            "Мои родители из России",
        ]
        personal_retriever = FAISS.from_texts(personal_texts, OpenAIEmbeddings()).as_retriever()

        # Добавляем информацию и описание для наших векторных хранилищ (баз знаний)

        retriever_infos = [
            {
                "name": "UII history",
                "description": "Подходит для ответов на вопросы о истории Гарант",
                "retriever": history_retriever  # Это база знаний из файла: 'UII_history_120623.txt'
            },
            {
                "name": "UII learning",
                "description": "Подходит для ответов на вопросы об обучении в Гарант",
                "retriever": learning_retriever  # Это база знаний из файла: 'UII_learning_120623.txt'
            },
            {
                "name": "personal",
                "description": "Походит для ответов обо мне и моем прошлом",
                "retriever": personal_retriever
                # Это база знаний из тектса: "Я люблю яблочный пирог (шарлотку)", "Мой любимый цвет фуксии", "Моя мечта стать проффесииональным танцором", "Я сломал руку, когда мне было 12",
                "Мои родители из России"
            }
        ]

        # Иициализируем класс

        chain = MultiRetrievalQAChain.from_retrievers(ChatOpenAI(temperature=0), retriever_infos, verbose=True)

        # Запускаем на вопросе из раздела истории Гарант (в промежуточных итогах видим что выбрано база знаний UII history)

        print(chain.run("Когда была получена первая заявка?"))

        # Запускаем на вопросе из раздела истории Гарант (в промежуточных итогах видим что выбрано база знаний UII learning)

        print(chain.run("Когда сдаются экзамены?"))

        # Запускаем на вопросе из раздела истории Гарант (в промежуточных итогах видим что выбрано база знаний personal)

        print(chain.run("Что было в моем прошлом?"))

        """## 3. Агенты (Agents)
        Агент имеет доступ к набору инструментов и определяет, какие из них использовать, в зависимости от пользовательского ввода.

        https://python.langchain.com/docs/modules/agents/
        """

        # @title Сервисные функции
        from langchain.agents import load_tools
        from langchain.agents import initialize_agent
        from langchain.agents import AgentType
        from langchain.llms import OpenAI
        from langchain.chat_models import ChatOpenAI

        llm = ChatOpenAI(temperature=0)

        """**Open Meteo API** - Полезно, когда вы хотите получить информацию о погоде из Open Meteo API. Входными данными должен быть вопрос на естественном языке, на который может ответить этот API.

        **llm-math или Калькулятор** - Полезно, когда вам нужно ответить на вопросы по математике (используется библиотека math)
        """

        # Определяем наши инструменты
        tools = load_tools(["open-meteo-api", "llm-math"], llm=llm)

        """ZERO_SHOT_REACT_DESCRIPTION - Этот агент использует ReAct фреймворк для определения, какой инструмент использовать, исключительно на основе описания инструмента. Может быть предоставлено любое количество инструментов. Этот агент требует, чтобы для каждого инструмента было предоставлено описание.

        **Примечание: Это агент действия наиболее общего назначения.**

        Полный список типов агентов см. в разделе https://python.langchain.com/docs/modules/agents/agent_types/

        verbose - это параметр отоборажения или скрытия промежуточной информации
        """

        # Инициализируем агента
        agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

        """Можно справшивать по отдельности, и в зависимости от вопросов будет использоваться тот или иной инструмент (у нас в данном случае есть инструмент API метео сервиса для информации о погоде, и математический основанный на библиотеке math)"""

        # Запускаем на вопросе про погоду (в промежуточных итогах видим что выбрано действие Action: Open Meteo API)

        agent.run("Какая погода в Сочи и скорость ветра? Ответь на руссском языке")

        # Запускаем на задачке по математике (в промежуточных итогах видим что выбрано действие Action: Calculator)

        agent.run(
            "Пол комнаты, имеющей форму прямоугольника со сторонами 3 м и строной 9 м, требуется покрыть паркетом из прямоугольных дощечек со сторонами 5 см и 30 см. Сколько потребуется таких дощечек? Ответь на руссском языке")

        """Можно в одном запросе перечислить несколько вопросов из одной или разных тем, соответственно применится тот или иной инструмент."""

        agent.run(
            "Сколько я потрачу бензина, при расходе моего автомобиля 7 л на 100 км в загородном режиме и 9.5 л на 100 км в городском режиме чтобы проехать 874 км по трассе и 125 км по городу? Какая погода в Сочи?")

        agent.run("Какая сейчас погода в Сочи? Какая погода сегодня в Пхукете Тайланд?")

        """## 4. HTTP Chain
        Использование библиотеки request для получения результатов HTML из URL, а затем передачу в LLM для анализа результатов.

        https://python.langchain.com/docs/modules/chains/additional/llm_requests
        """

        # @title Сервисные функции
        from langchain.chat_models import ChatOpenAI
        from langchain.chains import LLMRequestsChain, LLMChain
        from langchain.prompts import PromptTemplate

        def get_input(question: str):
            inputs = {
                "query": question,
                "url": "https://www.google.com/search?q=" + question.replace(" ", "+"),
            }
            return inputs

        """Поиск в Google (на Yandex не работает, надо изменить template под используемый парсер)"""

        # Подготавливаем темплейт для формирования запроса и получения ответа

        template = """Between >>> and <<< are the raw search result text from google.
        Extract the answer to the question '{query}' or say "not found" if the information is not contained.
        Use the format
        Extracted:<answer or "not found">
        >>> {requests_result} <<<
        Extracted:"""

        PROMPT = PromptTemplate(
            input_variables=["query", "requests_result"],
            template=template,
        )

        # Инициализируем класс

        chain = LLMRequestsChain(llm_chain=LLMChain(llm=ChatOpenAI(temperature=0), prompt=PROMPT))

        # Запускаем на вопросе

        question = get_input("Напиши три (3) крупнейшие страны и их размеры в км2?")

        result = chain(question)
        print(result)

        # Вывод только ответа

        print(result['output'])

        question = get_input("Основатель Университета исскуственного интеллекта?")

        result = chain(question)
        print(result)

        print(result['output'])

        """## 5. SQL
        Пример демонстрирует использование SQLDatabaseChain для ответов на вопросы через базу данных SQL и ее изменения (удаление, редактирование, добавление записей) с помощью запросов на естественном языке.

        https://python.langchain.com/docs/modules/chains/popular/sqlite

        Загрузка демонстрационной базы данных Chinook.

        База данных Chinook была создана как альтернатива базе данных Northwind. Она представляет собой хранилище цифровых мультимедиа, включающее таблицы для исполнителей, альбомов, мультимедийных дорожек, счетов-фактур и клиентов.
        """

        # @title Сервисные функции
        from langchain import OpenAI, SQLDatabase, SQLDatabaseChain
        from langchain.chat_models import ChatOpenAI

        # Отключение трасировки предупреждающих сообщений при выполнении кода
        os.environ["LANGCHAIN_TRACING"] = "false"

        """
        !wget
        https: // www.sqlitetutorial.net / wp - content / uploads / 2018 / 03 / chinook.zip
        !unzip
        chinook.zip
        """
        db = SQLDatabase.from_uri("sqlite://///content/chinook.db")
        llm = ChatOpenAI(temperature=0, verbose=True)

        # Инициализируем класс

        db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)

        # Вывод информации о таблицах базы данных Chinook

        print(db.table_info)

        # Запускаем на запросе удаления удаления данных из таблицы (из словесного описания сформирован sql запрос (SQLQuery:) на удаление)

        db_chain.run("Удали первую запись из таблицы работников?")

        # Запускаем на запросе о получении информации из таблицы (из словесного описания сформирован sql запрос (SQLQuery:) на выборку информации)

        db_chain.run("Назовите несколько примеров композиций Bach?")

        # Запускаем на запросе о получении информации из таблицы (из словесного описания сформирован sql запрос (SQLQuery:) на выборку информации)

        db_chain.run("Сколько альбомов у Aerosmith?")

        # Запускаем на запросе о получении информации из таблицы (из словесного описания сформирован sql запрос (SQLQuery:) на выборку информации)

        db_chain.run("Сколько сотрудников приживает в Канаде?")

        # Вариант вывода результата без промежуточной информации

        db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=False)
        db_chain.run("Сколько сотрудников приживает в Канаде?")

        """## 6. Поиск информации для ответа в разговоре QA и документу"""

        # @title Сервисные функции
        from langchain.embeddings.openai import OpenAIEmbeddings
        from langchain.vectorstores import Chroma, FAISS
        from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
        from langchain.chains import ConversationalRetrievalChain
        from langchain.chat_models import ChatOpenAI
        from langchain.document_loaders import TextLoader
        from langchain.memory import ConversationBufferMemory

        # Загрузка документа История Гарант
        """
        !wget - O
        UII_history_120623.txt
        "https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/fEwN40_ieKi_0g"
        """
        # Создание векторной базы из документа История Гарант
        loader = TextLoader("/content/UII_history_120623.txt")
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        documents = text_splitter.split_documents(documents)

        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(documents, embeddings)

        # Инициализация памяти истории чата
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

        def get_input(question: str):
            inputs = {"question": query}
            return inputs

        """### 1) Цепочка ConversationalRetrievalQA основана на цепочке RetrievalQAChain для обеспечения компонента истории чата.

        Сначала он объединяет историю чата (либо явно переданную, либо извлеченную из предоставленной памяти) и вопрос в отдельный вопрос, затем просматривает соответствующие документы у ретривера и, наконец, передает эти документы и вопрос в цепочку ответов на вопросы, чтобы вернуть ответ.
        """

        # Инициализируем класс

        qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0), vectorstore.as_retriever(), memory=memory)

        # Запускаем на вопросе

        question = get_input("Привет! Меня зовут Аркадий Аркадьевич, в каком году запущена певая версия Terra AI?")
        result = qa(question)

        # Вывод результата ответа модели

        print(result["answer"])

        # Просмотр результата включая промежуточные данные и историю сообщений

        result

        question = get_input("Какие книги нравятся Дмитрию Романову?")
        result = qa(question)

        print(result["answer"])

        # Просмотр результата включая промежуточные данные и историю сообщений

        result

        question = get_input("Какими языками программмирования он владеет?")
        result = qa(question)

        print(result["answer"])

        # Просмотр результата включая промежуточные данные и историю сообщений

        result

        """### 2) Использование разных моделей для уточнения вопроса и генерации ответа

        Эта цепочка состоит из двух этапов. Во-первых, она объединяет текущий вопрос и историю чата в отдельный вопрос. Это необходимо для создания отдельного вектора для использования при поиске. После этого он выполняет поиск, а затем отвечает на вопрос, используя расширенную генерацию поиска с помощью отдельной модели.

        GPT4 для генерации ответа, а ChatGpt (более дешевая)  для уточнения вопроса и отбора информации из истории и документа
        """

        # Инициализируем класс

        qa = ConversationalRetrievalChain.from_llm(
            ChatOpenAI(temperature=0, model="gpt-4"),
            vectorstore.as_retriever(),
            condense_question_llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),
        )
        # Инициализируем историю чата

        chat_history = []

        # Запускаем на вопросе

        question = "Привет! Меня зовут Аркадий Аркадьевич, в каком году запущена певая версия Terra AI?"
        result = qa({"question": question, "chat_history": chat_history})

        print(result["answer"])

        result

        chat_history.append((question, result["answer"]))
        question = "Какие книги нравятся Дмитрию Романову?"
        result = qa({"question": question, "chat_history": chat_history})

        print(result["answer"])

        result

        chat_history.append((question, result["answer"]))
        question = "Какими языками программмирования он владеет?"
        result = qa({"question": question, "chat_history": chat_history})

        print(result["answer"])

        result

        chat_history.append((question, result["answer"]))
        question = "Почему он провел первую живую лекцию сам?"
        result = qa({"question": question, "chat_history": chat_history})

        print(result["answer"])

        result

        """## 7. GPT4 извлечение сущностей
        Цепочка извлечения использует параметр OpenAI functions, чтобы указать схему для извлечения объектов из документа. Это помогает нам убедиться, что модель выводит именно ту схему объектов и свойств, которые мы хотим, с их соответствующими типами.

        Цепочку извлечения следует использовать, когда мы хотим извлечь несколько объектов с их свойствами из одного и того же отрывка (т. Е. какие люди были упомянуты в этом отрывке?)
        """

        # @title Сервисные функции
        from langchain.chat_models import ChatOpenAI
        from langchain.chains import create_extraction_chain, create_extraction_chain_pydantic
        from langchain.prompts import ChatPromptTemplate

        from typing import Optional, List
        from pydantic import BaseModel, Field

        llm = ChatOpenAI(temperature=0, model="gpt-4")

        # Либо с помощью ChatGpt
        # llm = ChatOpenAI(temperature=0, model="gpt-3.5")

        # Это наш текст из которого мы будем извлекать именованные сущности

        input = """
        Рост Владимира - 187 см. Евгения на 7 см выше Владимира и прыгает выше него. Евгения - брюнетка, а Владимир - блондин.
        Собака Владимира Конни - лабрадор, и она любит играть в прятки.
                """

        """### 1) **с помощью схемы**

        Чтобы извлечь сущности, нам нужно создать схему, подобную следующей, в которой мы указываем все свойства, которые мы хотим найти, и тип, который мы ожидаем, что они будут иметь. Мы также можем указать, какие из этих свойств являются обязательными, а какие необязательными.
        """

        # Создаем схему структуры наших параметров

        schema = {
            "properties": {
                "person_name": {"type": "string"},
                "person_height": {"type": "integer"},
                "person_hair_color": {"type": "string"},
                "dog_name": {"type": "string"},
                "dog_breed": {"type": "string"},
            },
            "required": ["person_name", "person_height"],
        }

        chain = create_extraction_chain(schema, llm)

        # Запускаем на тексте (получаем структуру найденных сущностей согласно созданной схеме)

        chain.run(input)

        """### 2) **с помощью pydantic**

        Мы также можем использовать схему Pydantic для выбора требуемых свойств и типов, и мы установим как "Необязательные" те, которые не являются строго обязательными.

        Используя create_extraction_chain_pydantic функцию, мы можем отправить Pydantic schema в качестве входных данных, а на выходе будет создан экземпляр объекта, который соответствует нашей желаемой схеме.

        Таким образом, мы можем указать нашу схему таким же образом, как мы определили бы новый класс или функцию в Python - с чисто питоновскими типами.
        """

        # Создаем класс наших параметров

        class Properties(BaseModel):
            person_name: str
            person_height: int
            person_hair_color: str
            dog_breed: Optional[str]
            dog_name: Optional[str]

        chain = create_extraction_chain_pydantic(pydantic_schema=Properties, llm=llm)

        # Запускаем на тексте (получаем типизированную структуру)

        chain.run(input)

        """## 8. Чат с GPT4"""

        # @title Сервисные функции
        from langchain.memory import ConversationBufferMemory
        from langchain.chat_models import ChatOpenAI
        from langchain.chains import ConversationChain

        llm = ChatOpenAI(temperature=0, model="gpt-4")

        # Установка verbose в True приведет к распечатке некоторых внутренних состояний Chain объекта во время его запуска.
        conversation = ConversationChain(
            llm=llm,
            memory=ConversationBufferMemory(),
            verbose=True
        )

        # Запуск диалога с нашим вопросом (в промежуточном выводе сохраняется диалог)

        conversation.run("Привет я Анатолий. Что такое ChatGpt?")

        conversation.run("Как ее можно использовать?")

        conversation.run("Напомни как меня зовут?")

        """## 9. Вызов функции в OpenAI
        В вызове API вы можете описать функции в gpt-3.5-turbo-0613 и gpt-4-0613 и заставить модель разумно выбрать вывод объекта JSON, содержащего аргументы для вызова этих функций. API не вызывает функцию; вместо этого модель генерирует JSON, который вы можете использовать для вызова функции в своем коде.


        **Пример использования выделения сущностей из текста и  вызова функций в моделях OpenAi ведения таблицы о ваших тратах (покупках)**
        """

        # @title Сервисные функции
        import openai
        import json
        import pandas as pd

        # Пример фиктивной функции, запрограммированной на добавление записи о ваших расходах
        # В рабочей среде это может быть ваш внутренний API или внешний API
        def add_current_sale(location, count, product_name):
            """Добавить запись в таблицу расходов"""
            df.loc[len(df.index)] = [location, count, product_name]
            sales_info = {
                "Место покупки": location,
                "Количество": count,
                "Наименование": product_name,
                "Статус": "Записано!"
            }
            df.head()
            return json.dumps(sales_info)

        def run_conversation(query):
            # Шаг 1: отправьте разговор и доступные функции в GPT
            messages = [{"role": "system", "content": "Ты вежливый Ассистент."},
                        {"role": "user", "content": query}]

            functions = [
                {
                    "name": "add_current_sale",
                    "description": "Добавить запись в таблицу расходов.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "Место покупки": {
                                "type": "string",
                                "description": "Место где потрачены деньги, например магазин (Пятерочка), пекарня, заправка и т.д.",
                            },
                            "Количество": {"type": "string",
                                           "description": "Количество купленного товара, например 1, 5 бутылок, 4 куска, 2 штуки, 1 пакет и т.д.",
                                           },

                            "Наименование": {"type": "string",
                                             "description": "Наименование купленного товара, например вода без газа, минералка Есентуки, масло сливочное, бензин и т.д.",
                                             },
                        },
                        "required": ["Место покупки", "Количество", "Наименование"],
                    },
                },
            ]
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages,
                functions=functions,
                function_call="auto",  # по умолчанию используется значение auto
            )
            response_message = response["choices"][0]["message"]

            # Шаг 2: проверьте, хочет ли GPT вызвать функцию
            if response_message.get("function_call"):
                # Шаг 3: вызовите функцию
                # Примечание: ответ JSON не всегда может быть действительным; обязательно обработайте ошибки
                available_functions = {
                    "add_current_sale": add_current_sale,
                }  # в этом примере только одна функция, но у вас может быть несколько
                function_name = response_message["function_call"]["name"]
                fuction_to_call = available_functions[function_name]
                function_args = json.loads(response_message["function_call"]["arguments"])
                print(function_args)
                function_response = fuction_to_call(
                    location=function_args.get("Место покупки"),
                    count=function_args.get("Количество"),
                    product_name=function_args.get("Наименование"),
                )

                # Шаг 4: отправьте информацию о вызове функции и ответе функции в GPT
                messages.append(response_message)  # продлите разговор с помощью ответа помощника
                messages.append(
                    {
                        "role": "function",
                        "name": function_name,
                        "content": function_response,
                    }
                )  # расширьте диалог с помощью function_response
                second_response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=messages,
                )  # получите новый ответ от GPT, где вы сможете увидеть ответ функции
                return second_response["choices"][0]["message"]["content"]
            return response_message["content"]

        """Создадим тестовые данные"""

        # Создадим тестовую таблицу расходов с некоторыми данными

        data = [
            ["Пекарня", '5 штук', "круасаны"],
            ["Пятерочка", '1 бутылка', "вода без газа"]
        ]

        df = pd.DataFrame(data, columns=["Место покупки", "Количество", "Наименование"])
        df

        """Пример добавления записи о ваших расходах в dataframe Pandas"""

        # Запуск модели с вашими данными о покупках (в промежуточном выводе отображается структура найденных данных и ответ модели)

        print(run_conversation('Я сегодня был в Пятерочке, и купил 7 бутылок кефира. Добавь в таблицу расходов'))

        # Просмотр таблицы на наличие новой записи

        df

        """Данный запрос не вызывает нашу функцию а просто генерирует ответ"""

        # Запуск модели с данными не относящимися к покупкам (в промежуточном выводе отображается только ответ модели)

        print(run_conversation('Напиши пару строк про искуственный интеллект'))

        # Просмотр таблицы на отсутствие новой записи

        df

        # Запуск модели с вашими данными о покупках (в промежуточном выводе отображается структура найденных данных и ответ модели)

        print(run_conversation('Весь день пробыл в дороге, заезжал на заправку, и купил 41 литр бензина.'))

        # Просмотр таблицы на наличие новой записи

        df


if __name__ == '__main__':
    draft05_litetask().run("")